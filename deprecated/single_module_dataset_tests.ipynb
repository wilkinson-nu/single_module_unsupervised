{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "writer = SummaryWriter(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = [8, 6]\n",
    "mpl.rcParams['font.size'] = 16\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.device(device)\n",
    "import numpy as np\n",
    "SEED=12345\n",
    "_=np.random.seed(SEED)\n",
    "_=torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Make the single module dataset\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import numpy as np\n",
    "import joblib\n",
    "import scipy\n",
    "from glob import glob\n",
    "from itertools import chain\n",
    "\n",
    "class SingleModuleImage2D_sparse_joblib(Dataset):\n",
    "\n",
    "    def __init__(self, infilename, normalize=None, transform=None):\n",
    "        \n",
    "        self._data = joblib.load(infilename)\n",
    "        self._length = len(self._data)\n",
    "        self._normalize = normalize\n",
    "        self._transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        ## Convert to a dense pytorch tensor...\n",
    "        # data = torch.Tensor(self._data[idx].toarray())\n",
    "        data = self._data[idx].toarray()\n",
    "        \n",
    "        if self._transform:\n",
    "            data = self._transform(data)\n",
    "        \n",
    "        data = torch.Tensor(data) #self._data[idx].toarray())\n",
    "        \n",
    "        ## Normalize entries if necessary\n",
    "        ## Various possibilities here. Should the sum be equal to 1, or should the maximum value in the image be 1?\n",
    "        ## Alternatively, maybe I should transform it so the sqrt of all values is used (to amplify the small features?)\n",
    "        if self._normalize:\n",
    "            # data = data/np.amax(data.numpy())\n",
    "            data = data -1.2 #/np.sum(data.numpy())\n",
    "\n",
    "        ## By default, this is assumed to be in \"Tensor, label\" format. The collate function is necessary because this is different...\n",
    "        return data\n",
    "\n",
    "def collate(batch):\n",
    "    batched_data = torch.cat([sample[None][None] for sample in batch],0)\n",
    "    return batched_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "def sqrt_transform(x):\n",
    "    return np.sqrt(x)\n",
    "def cbrt_transform(x):\n",
    "    return np.cbrt(x)\n",
    "def log_transform(x):\n",
    "    return np.log10(1+x)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(log_transform)\n",
    "])\n",
    "\n",
    "## Get a concrete dataset and data loader\n",
    "inFile = \"/global/cfs/cdirs/dune/users/cwilk/single_module_images/sparse_joblib_fixdupes_pluscuts_noneg/training_images_200k.joblib\"\n",
    "## Small version\n",
    "inFile = \"/global/cfs/cdirs/dune/users/cwilk/single_module_images/sparse_joblib_fixdupes_pluscuts_noneg_transform/packet_2022_02_07_23_09_05_CET_0cd913fb_20220207_230906.data.module1_flow_images.joblib\"\n",
    "start = time.process_time() \n",
    "train_dataset = SingleModuleImage2D_sparse_joblib(inFile, False) #, transform=transform)\n",
    "print(\"Time taken to load\", train_dataset.__len__(),\"images:\", time.process_time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Randomly chosen batching\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           collate_fn=collate,\n",
    "                                           batch_size=128,\n",
    "                                           shuffle=True, \n",
    "                                           num_workers=4,\n",
    "                                           drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = time.process_time()\n",
    "minVal = 0\n",
    "maxVal = 0\n",
    "for image_batch in train_loader:\n",
    "    if torch.max(image_batch) > maxVal:\n",
    "        maxVal = torch.max(image_batch)\n",
    "    if torch.min(image_batch) < minVal:\n",
    "        minVal = torch.min(image_batch)\n",
    "print(\"Time taken to loop:\", time.process_time() - start)\n",
    "print(\"Found a minimum value of:\", minVal)\n",
    "print(\"Found a maximum value of:\", maxVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Visualise data\n",
    "# Access a specific instance\n",
    "data = train_dataset[2]\n",
    "print(data.size())\n",
    "# The data instance is a dictionary\n",
    "# print('List of keys in a data element',data.keys(),'\\n')\n",
    "\n",
    "# Visualize the image\n",
    "gr = plt.imshow(data, origin='lower')\n",
    "plt.colorbar(gr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Define the encoder and decoders that do the business\n",
    "from torch import nn\n",
    "class EncoderSimple(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 base_channel_size : int,\n",
    "                 latent_dim : int,\n",
    "                 act_fn : object = nn.LeakyReLU):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            - base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "            - latent_dim : Dimensionality of latent representation z\n",
    "            - act_fn : Activation function used throughout the encoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        n_chan = base_channel_size\n",
    "        \n",
    "        ### Convolutional section\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            ## Note the assumption that the input image has a single channel\n",
    "            nn.Conv2d(in_channels=1, \n",
    "                      out_channels=n_chan, \n",
    "                      kernel_size=3, stride=2, padding=1), ## 280x140 ==> 140x70\n",
    "            act_fn(),\n",
    "            nn.Conv2d(in_channels=n_chan, \n",
    "                      out_channels=n_chan, \n",
    "                      kernel_size=3, padding=1), ## No change in size\n",
    "            act_fn(),\n",
    "            nn.Conv2d(in_channels=n_chan, \n",
    "                      out_channels=n_chan, \n",
    "                      kernel_size=3, padding=1), ## No change in size\n",
    "            act_fn(),\n",
    "            nn.Conv2d(in_channels=n_chan, \n",
    "                      out_channels=2*n_chan, \n",
    "                      kernel_size=3, stride=2, padding=1), ## 140x70 ==> 70x35\n",
    "            act_fn(),\n",
    "            nn.Conv2d(in_channels=2*n_chan, \n",
    "                      out_channels=2*n_chan, \n",
    "                      kernel_size=3, padding=1), ## No change in size\n",
    "            act_fn(),\n",
    "            nn.Conv2d(in_channels=2*n_chan, \n",
    "                      out_channels=2*n_chan, \n",
    "                      kernel_size=3, padding=1), ## No change in size\n",
    "            act_fn(),\n",
    "            nn.Conv2d(in_channels=2*n_chan, \n",
    "                      out_channels=4*n_chan, \n",
    "                      kernel_size=3, stride=2, padding=(1,0)), ## 35x17\n",
    "            act_fn(),\n",
    "            ## Add a 1x1 convolution to reduce the number of layers here\n",
    "            #nn.Conv2d(in_channels=4*n_chan, out_channels=n_chan, kernel_size=1),\n",
    "            #act_fn()\n",
    "        )\n",
    "        \n",
    "        ### Flatten layer\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        \n",
    "        ### Linear section, simple for now\n",
    "        ## This is 8960...\n",
    "        self.encoder_lin = nn.Sequential(\n",
    "            ## Number of nodes in last layer (16*n_chan) multiplied by number of pixels in deepest layer (4x4)\n",
    "            nn.Linear(4*n_chan*35*17, 1000),\n",
    "            #act_fn(),      \n",
    "            #nn.Linear(1000,500),\n",
    "            act_fn(),      \n",
    "            nn.Linear(1000,latent_dim)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights using Xavier initialization\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder_cnn(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.encoder_lin(x)\n",
    "        return x\n",
    "    \n",
    "class DecoderSimple(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 base_channel_size : int,\n",
    "                 latent_dim : int,\n",
    "                 act_fn : object = nn.LeakyReLU):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            - base_channel_size : Number of channels we use in the last convolutional layers. Early layers might use a duplicate of it.\n",
    "            - latent_dim : Dimensionality of latent representation z\n",
    "            - act_fn : Activation function used throughout the decoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        n_chan = base_channel_size\n",
    "\n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 1000),\n",
    "            act_fn(),\n",
    "            #nn.Linear(500, 1000),\n",
    "            #act_fn(),\n",
    "            nn.Linear(1000, 4*n_chan*35*17),\n",
    "            act_fn()\n",
    "        )\n",
    "\n",
    "        self.unflatten = nn.Unflatten(dim=1, \n",
    "        unflattened_size=(4*n_chan, 35, 17))\n",
    "\n",
    "        self.decoder_conv = nn.Sequential(  \n",
    "            nn.ConvTranspose2d(in_channels=4*n_chan, \n",
    "                               out_channels=2*n_chan, \n",
    "                               kernel_size=3, stride=2, padding=(1,0), output_padding=(1,0)), ## 35x17 ==> 70x35\n",
    "            act_fn(),\n",
    "            nn.Conv2d(in_channels=2*n_chan,\n",
    "                      out_channels=2*n_chan,\n",
    "                      kernel_size=3, padding=1), ## No change in size\n",
    "            act_fn(), \n",
    "            nn.Conv2d(in_channels=2*n_chan,\n",
    "                      out_channels=2*n_chan,\n",
    "                      kernel_size=3, padding=1), ## No change in size\n",
    "            act_fn(), \n",
    "            nn.ConvTranspose2d(in_channels=2*n_chan, \n",
    "                               out_channels=n_chan, \n",
    "                               kernel_size=3, stride=2, padding=1, output_padding=1), ## 70x35 ==> 140x70\n",
    "            act_fn(),\n",
    "            nn.Conv2d(in_channels=n_chan,\n",
    "                      out_channels=n_chan,\n",
    "                      kernel_size=3, padding=1), ## No change in size\n",
    "            act_fn(),\n",
    "            nn.Conv2d(in_channels=n_chan,\n",
    "                      out_channels=n_chan,\n",
    "                      kernel_size=3, padding=1), ## No change in size\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(in_channels=n_chan, \n",
    "                               out_channels=1, \n",
    "                               kernel_size=3, stride=2, padding=1, output_padding=1), ## 140x70 ==> 280x140\n",
    "            act_fn()\n",
    "        )\n",
    "        \n",
    "        # Initialize weights using Xavier initialization\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.decoder_lin(x)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.decoder_conv(x)\n",
    "        # x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class EncoderDeep(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 base_channel_size : int,\n",
    "                 latent_dim : int,\n",
    "                 act_fn : object = nn.LeakyReLU,\n",
    "                 drop_fract : float = 0.2):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            - base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "            - latent_dim : Dimensionality of latent representation z\n",
    "            - act_fn : Activation function used throughout the encoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        n_chan = base_channel_size\n",
    "        \n",
    "        ### Convolutional section\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            ## Note the assumption that the input image has a single channel\n",
    "            nn.Conv2d(in_channels=1, out_channels=n_chan, kernel_size=3, stride=2, padding=1), ## 280x140 ==> 140x70\n",
    "            nn.BatchNorm2d(n_chan),\n",
    "            #nn.LayerNorm([n_chan, 140, 70]),\n",
    "            act_fn(),\n",
    "            nn.Dropout(drop_fract),\n",
    "            #nn.Conv2d(in_channels=n_chan, out_channels=n_chan, kernel_size=3, padding=1), ## No change in size\n",
    "            nn.BatchNorm2d(n_chan),\n",
    "            #nn.LayerNorm([n_chan, 140, 70]),\n",
    "            act_fn(),\n",
    "            # nn.Dropout(drop_fract),\n",
    "            nn.Conv2d(in_channels=n_chan, out_channels=n_chan, kernel_size=3, padding=1), ## No change in size\n",
    "            nn.BatchNorm2d(n_chan),\n",
    "            #nn.LayerNorm([n_chan, 140, 70]),          \n",
    "            act_fn(),\n",
    "            nn.Dropout(drop_fract),\n",
    "            nn.Conv2d(in_channels=n_chan, out_channels=2*n_chan, kernel_size=3, stride=2, padding=1), ## 140x70 ==> 70x35\n",
    "            nn.BatchNorm2d(2*n_chan),\n",
    "            #nn.LayerNorm([2*n_chan, 70, 35]),\n",
    "            act_fn(),\n",
    "            nn.Dropout(drop_fract),\n",
    "            nn.Conv2d(in_channels=2*n_chan, out_channels=2*n_chan, kernel_size=3, padding=1), ## No change in size\n",
    "            nn.BatchNorm2d(2*n_chan),\n",
    "            #nn.LayerNorm([2*n_chan, 70, 35]),\n",
    "            act_fn(),\n",
    "            nn.Dropout(drop_fract),\n",
    "            nn.Conv2d(in_channels=2*n_chan, out_channels=2*n_chan, kernel_size=3, padding=1), ## No change in size\n",
    "            nn.BatchNorm2d(2*n_chan),\n",
    "            #nn.LayerNorm([2*n_chan, 70, 35]),\n",
    "            act_fn(),\n",
    "            nn.Dropout(drop_fract),\n",
    "            nn.Conv2d(in_channels=2*n_chan, out_channels=4*n_chan, kernel_size=3, stride=2, padding=1), ## 70x35 ==> 35x18\n",
    "            nn.BatchNorm2d(4*n_chan),\n",
    "            #nn.LayerNorm([4*n_chan, 35, 18]),\n",
    "            act_fn(),\n",
    "            nn.Dropout(drop_fract),\n",
    "            nn.Conv2d(in_channels=4*n_chan, out_channels=4*n_chan, kernel_size=3, padding=1), ## No change in size\n",
    "            nn.BatchNorm2d(4*n_chan),\n",
    "            #nn.LayerNorm([4*n_chan, 35, 18]),\n",
    "            #act_fn(),\n",
    "            nn.Dropout(drop_fract),\n",
    "            nn.Conv2d(in_channels=4*n_chan, out_channels=4*n_chan, kernel_size=3, padding=1), ## No change in size\n",
    "            nn.BatchNorm2d(4*n_chan),\n",
    "            #nn.LayerNorm([4*n_chan, 35, 18]),\n",
    "            act_fn(),\n",
    "            nn.Dropout(drop_fract),\n",
    "            nn.Conv2d(in_channels=4*n_chan, out_channels=8*n_chan, kernel_size=3, stride=2, padding=1), ## 35x18 ==> 18x9\n",
    "            nn.BatchNorm2d(8*n_chan),\n",
    "            #nn.LayerNorm([8*n_chan, 18, 9]),\n",
    "            act_fn(),\n",
    "            nn.Dropout(drop_fract)\n",
    "\n",
    "        )\n",
    "        \n",
    "        ### Flatten layer\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        \n",
    "        ### Linear section, simple for now\n",
    "        self.encoder_lin = nn.Sequential(\n",
    "            ## Number of nodes in last layer multiplied by number of pixels in deepest layer\n",
    "            nn.Linear(8*n_chan*18*9, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            act_fn(),\n",
    "            # nn.Dropout(drop_fract),\n",
    "            #nn.Linear(512, 512),\n",
    "            # nn.BatchNorm1d(1000),\n",
    "            #act_fn(),\n",
    "            ## https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Understanding_the_Disharmony_Between_Dropout_and_Batch_Normalization_by_Variance_CVPR_2019_paper.pdf\n",
    "            ## This paper suggests that dropout and batchnorm don't play well, but adding dropout at the last stage can help\n",
    "            nn.Dropout(drop_fract),\n",
    "            nn.Linear(1024, latent_dim),\n",
    "            # act_fn()\n",
    "        )\n",
    "        \n",
    "        # Initialize weights using Xavier initialization\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder_cnn(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.encoder_lin(x)\n",
    "        return x\n",
    "    \n",
    "class DecoderDeep(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 base_channel_size : int,\n",
    "                 latent_dim : int,\n",
    "                 act_fn : object = nn.LeakyReLU):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            - base_channel_size : Number of channels we use in the last convolutional layers. Early layers might use a duplicate of it.\n",
    "            - latent_dim : Dimensionality of latent representation z\n",
    "            - act_fn : Activation function used throughout the decoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        n_chan = base_channel_size\n",
    "\n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 1024),\n",
    "            # nn.BatchNorm1d(1000),\n",
    "            #act_fn(),\n",
    "            #nn.Linear(512, 512),\n",
    "            act_fn(),\n",
    "            nn.Linear(1024, 8*n_chan*18*9),\n",
    "            # nn.BatchNorm1d(8*n_chan*17*8),\n",
    "            act_fn()\n",
    "        )\n",
    "\n",
    "        self.unflatten = nn.Unflatten(dim=1, \n",
    "        unflattened_size=(8*n_chan, 18, 9))\n",
    "\n",
    "        self.decoder_conv = nn.Sequential(  \n",
    "            nn.ConvTranspose2d(in_channels=8*n_chan, out_channels=4*n_chan, kernel_size=3, stride=2, padding=1, output_padding=(0,1)), ## 18x9 ==> 35x18\n",
    "            nn.BatchNorm2d(4*n_chan),\n",
    "            #nn.LayerNorm([4*n_chan, 35, 18]),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(in_channels=4*n_chan, out_channels=4*n_chan, kernel_size=3, padding=1), ## No change in size\n",
    "            nn.BatchNorm2d(4*n_chan),\n",
    "            #nn.LayerNorm([4*n_chan, 35, 18])\n",
    "            act_fn(),\n",
    "            nn.Conv2d(in_channels=4*n_chan, out_channels=4*n_chan, kernel_size=3, padding=1), ## No change in size\n",
    "            nn.BatchNorm2d(4*n_chan),\n",
    "            #nn.LayerNorm([4*n_chan, 35, 18]),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(in_channels=4*n_chan, out_channels=2*n_chan, kernel_size=3, stride=2, padding=1, output_padding=(1,0)), ## 35x18 ==> 70x35\n",
    "            nn.BatchNorm2d(2*n_chan),\n",
    "            #nn.LayerNorm([2*n_chan, 70, 35]),            \n",
    "            act_fn(),\n",
    "            #nn.Conv2d(in_channels=2*n_chan, out_channels=2*n_chan, kernel_size=3, padding=1), ## No change in size\n",
    "            nn.BatchNorm2d(2*n_chan),\n",
    "            #nn.LayerNorm([2*n_chan, 70, 35]),            \n",
    "            act_fn(), \n",
    "            nn.Conv2d(in_channels=2*n_chan, out_channels=2*n_chan, kernel_size=3, padding=1), ## No change in size\n",
    "            nn.BatchNorm2d(2*n_chan),\n",
    "            #nn.LayerNorm([2*n_chan, 70, 35]),                      \n",
    "            act_fn(), \n",
    "            nn.ConvTranspose2d(in_channels=2*n_chan, out_channels=n_chan, kernel_size=3, stride=2, padding=1, output_padding=1), ## 70x35 ==> 140x70\n",
    "            nn.BatchNorm2d(n_chan),\n",
    "            #nn.LayerNorm([n_chan, 140, 70]),            \n",
    "            act_fn(),\n",
    "            nn.Conv2d(in_channels=n_chan, out_channels=n_chan, kernel_size=3, padding=1), ## No change in size\n",
    "            nn.BatchNorm2d(n_chan),\n",
    "            #nn.LayerNorm([n_chan, 140, 70]),            \n",
    "            act_fn(),\n",
    "            nn.Conv2d(in_channels=n_chan, out_channels=n_chan, kernel_size=3, padding=1), ## No change in size\n",
    "            nn.BatchNorm2d(n_chan),\n",
    "            #nn.LayerNorm([n_chan, 140, 70]),                        \n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(in_channels=n_chan, out_channels=1, kernel_size=3, stride=2, padding=1, output_padding=1), ## 140x70 ==> 280x140\n",
    "            # nn.BatchNorm2d(1),\n",
    "            act_fn()\n",
    "        )\n",
    "        # Initialize weights using Xavier initialization\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.decoder_lin(x)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.decoder_conv(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_ae_outputs(encoder,decoder,n=10):  \n",
    "    \n",
    "    plt.figure(figsize=(12,6))\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    ## Loop over figures\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(3,n,i+1)\n",
    "        ## This is not working the way I expect when shuffle is not on...\n",
    "        ## It always gives the first image...\n",
    "        img = next(iter(train_loader))\n",
    "        with torch.no_grad():\n",
    "            img = img.to(device)\n",
    "            # temp=encoder(img)\n",
    "            rec_img  = decoder(encoder(img))\n",
    "        this_input  = img[0].cpu().numpy().squeeze()\n",
    "        this_output = rec_img[0].cpu().numpy().squeeze()\n",
    "        \n",
    "        ## Input row\n",
    "        plt.imshow(this_input, cmap='viridis', origin='lower')            \n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)  \n",
    "        if i == n//2: ax.set_title('Original images')\n",
    "        \n",
    "        ## Reconstructed row\n",
    "        ax = plt.subplot(3, n, i + 1 + n)\n",
    "        plt.imshow(this_output, cmap='viridis', origin='lower')  \n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)  \n",
    "        if i == n//2: ax.set_title('Reconstructed images')\n",
    "        \n",
    "        ## In - rec row\n",
    "        ax = plt.subplot(3, n, i + 1 + 2*n)\n",
    "        plt.imshow(this_input-this_output, cmap='viridis', origin='lower')  \n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)  \n",
    "        if i == n//2: ax.set_title('Input - reco images')\n",
    "        \n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to plot weight distribution\n",
    "def plot_weight_distribution(model):\n",
    "    # Collect all the weights from the model\n",
    "    all_weights = []\n",
    "    for param in model.parameters():\n",
    "        all_weights.extend(param.data.cpu().numpy().flatten())\n",
    "    \n",
    "    # Plot histogram of weights\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(all_weights, bins=50, color='blue', alpha=0.7)\n",
    "    plt.title('Weight Distribution')\n",
    "    plt.xlabel('Weight Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_distribution_from_dataloader(data_loader, encoder, decoder):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    # Initialize empty lists to store histogram counts\n",
    "    num_bins=50\n",
    "    input_hist = np.zeros(num_bins, dtype=int)\n",
    "    output_hist = np.zeros(num_bins, dtype=int)\n",
    "\n",
    "    # Create logarithmically spaced bins\n",
    "    # bins = np.logspace(0, np.log10(1000), num=num_bins+1)\n",
    "    bins = np.linspace(0.1, 3.1, num=num_bins+1)\n",
    "    with torch.no_grad():\n",
    "        for image_batch in data_loader:\n",
    "\n",
    "            image_batch = image_batch.to(device)\n",
    "            # Encode data\n",
    "            encoded_batch = encoder(image_batch)\n",
    "            # Decode data\n",
    "            decoded_batch = decoder(encoded_batch)\n",
    "            \n",
    "            # Flatten input and output tensors to 1D arrays\n",
    "            # Update input histogram\n",
    "            input_hist += np.histogram(image_batch.cpu().numpy(), bins=bins)[0]\n",
    "\n",
    "            # Update output histogram\n",
    "            output_hist += np.histogram(decoded_batch.cpu().numpy(), bins=bins)[0]\n",
    "\n",
    "    # Plot distribution of input and output values\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(bins[:-1], input_hist, color='blue', label='Input')\n",
    "    plt.plot(bins[:-1], output_hist, color='orange', label='Output')\n",
    "    plt.title('Distribution of Input and Output Values')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    # plt.xscale('log')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## This is a loss function to deweight the penalty for getting blank pixels wrong\n",
    "class AsymmetricLoss(torch.nn.Module):\n",
    "    def __init__(self, nonzero_cost=2.0, zero_cost=1.0, l1_weight=0):\n",
    "        super(AsymmetricLoss, self).__init__()\n",
    "        self.nonzero_cost = nonzero_cost\n",
    "        self.zero_cost = zero_cost\n",
    "        self.l1_weight = l1_weight\n",
    "        self.null_value = 0\n",
    "    \n",
    "    def forward(self, predictions, targets, encoder, decoder):\n",
    "        ## Calculate the absolute difference between predictions and targets\n",
    "        sq_err = (predictions - targets)**2\n",
    "        \n",
    "        ## Calculate the loss for nonzero values\n",
    "        nonzero = self.nonzero_cost * torch.where(targets != self.null_value, sq_err, torch.zeros_like(sq_err))\n",
    "        \n",
    "        ## Calculate the loss for predicting a nonzero value for zero\n",
    "        zero = self.zero_cost * torch.where(targets == self.null_value, torch.where(predictions != self.null_value, sq_err, torch.zeros_like(sq_err)), torch.zeros_like(sq_err))\n",
    "\n",
    "        ## Total loss is the sum of nonzero_loss and zero_loss\n",
    "        reco_loss = torch.mean(zero + nonzero)\n",
    "        \n",
    "        ## Add the L1 norm term\n",
    "        l1_norm = torch.tensor(0., device=predictions.device)\n",
    "        num_params = sum(p.numel() for p in encoder.parameters()) + sum(p.numel() for p in decoder.parameters())\n",
    "        for param in encoder.parameters(): l1_norm += torch.norm(param, p=1)\n",
    "        for param in decoder.parameters(): l1_norm += torch.norm(param, p=1)\n",
    "    \n",
    "        total_loss = reco_loss + self.l1_weight*reco_loss.item()*l1_norm/num_params\n",
    "        # print(\"Total loss =\", total_loss.item(), \"=\", reco_loss.item(), \" (reco) +\", l1_norm.item(), \"(l1)\", num_params)\n",
    "        \n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## This is a loss function to deweight the penalty for getting blank pixels wrong\n",
    "class AsymmetricL1Loss(torch.nn.Module):\n",
    "    def __init__(self, nonzero_cost=2.0, zero_cost=1.0, l1_weight=0):\n",
    "        super(AsymmetricL1Loss, self).__init__()\n",
    "        self.nonzero_cost = nonzero_cost\n",
    "        self.zero_cost = zero_cost\n",
    "        self.l1_weight = l1_weight\n",
    "    \n",
    "    def forward(self, predictions, targets, encoder, decoder):\n",
    "        ## Calculate the absolute difference between predictions and targets\n",
    "        diff = torch.abs(predictions - targets)\n",
    "        \n",
    "        ## Calculate the loss for nonzero values\n",
    "        nonzero = self.nonzero_cost * torch.where(targets != 0, diff, torch.zeros_like(diff))\n",
    "        \n",
    "        ## Calculate the loss for predicting a nonzero value for zero\n",
    "        zero = self.zero_cost * torch.where(targets == 0, torch.where(predictions != 0, diff, torch.zeros_like(diff)), torch.zeros_like(diff))\n",
    "\n",
    "        ## Total loss is the sum of nonzero_loss and zero_loss\n",
    "        reco_loss = torch.mean(zero + nonzero)\n",
    "        \n",
    "        ## Add the L1 norm term\n",
    "        l1_norm = torch.tensor(0., device=predictions.device)\n",
    "        num_params = sum(p.numel() for p in encoder.parameters()) + sum(p.numel() for p in decoder.parameters())\n",
    "        for param in encoder.parameters(): l1_norm += torch.norm(param, p=1)\n",
    "        for param in decoder.parameters(): l1_norm += torch.norm(param, p=1)\n",
    "    \n",
    "        total_loss = reco_loss + self.l1_weight*reco_loss.item()*l1_norm/num_params\n",
    "        # print(\"Total loss =\", total_loss.item(), \"=\", reco_loss.item(), \" (reco) +\", l1_norm.item(), \"(l1)\", num_params)\n",
    "        \n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Wrap the training in a nicer function...\n",
    "def run_training(num_iterations, log_dir, encoder, decoder, dataloader, optimizer, scheduler=None):\n",
    "\n",
    "    print(\"Training with\", num_iterations, \"iterations\")\n",
    "    tstart = time.time()\n",
    "\n",
    "    if log_dir:\n",
    "        writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    # Set train mode for both the encoder and the decoder\n",
    "    # encoder.train()\n",
    "    # decoder.train()\n",
    "\n",
    "    ## The loss function isn't going to change, so... don't\n",
    "    # loss_fn = torch.nn.L1Loss()\n",
    "    # loss_fn = torch.nn.MSELoss() \n",
    "    # loss_fn = torch.nn.SmoothL1Loss()\n",
    "    loss_fn = AsymmetricLoss(10, 1, 0) #1e-2)\n",
    "    # loss_fn2 = torch.nn.MSELoss() \n",
    "    ## Loop over the desired iterations\n",
    "    for iteration in range(num_iterations):\n",
    "        \n",
    "        total_loss = 0\n",
    "        # total_loss2 = 0\n",
    "        nbatches   = 0\n",
    "        \n",
    "        # Set train mode for both the encoder and the decoder\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "    \n",
    "        # Iterate over batches of images with the dataloader\n",
    "        for image_batch in dataloader:\n",
    "            print(image_batch.shape)\n",
    "\n",
    "            # Move tensor to the proper device\n",
    "            image_batch = image_batch.to(device)\n",
    "            # Encode data\n",
    "            encoded_batch = encoder(image_batch)\n",
    "            # Decode data\n",
    "            decoded_batch = decoder(encoded_batch)\n",
    "            # Evaluate loss\n",
    "            loss = loss_fn(decoded_batch, image_batch, encoder, decoder)\n",
    "\n",
    "            # loss2 = loss_fn2(decoded_batch, image_batch)\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()        \n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            # total_loss2 += loss2.item()\n",
    "            nbatches += 1\n",
    "        \n",
    "        ## See if we have an LR scheduler...\n",
    "        if scheduler: scheduler.step() #total_loss)\n",
    "        \n",
    "        av_loss = total_loss/nbatches\n",
    "        # av_loss2 = total_loss2/nbatches\n",
    "\n",
    "        if log_dir: \n",
    "            writer.add_scalar('loss/train', av_loss, iteration)\n",
    "        #if iteration%10 == 0:\n",
    "        print(\"Processed\", iteration, \"/\", num_iterations, \"; loss =\", av_loss) #, av_loss2)\n",
    "        print(\"Time taken:\", time.process_time() - start)\n",
    "        if iteration%20 == 0: \n",
    "            ## Plot how things look\n",
    "            plot_ae_outputs(encoder,decoder,10)\n",
    "            ## Also plot the weights for the encoder\n",
    "            #print(\"Plotting encoder weights\")\n",
    "            #plot_weight_distribution(encoder)\n",
    "            #print(\"Plotting decoder weights\")\n",
    "            #plot_weight_distribution(decoder)  \n",
    "            \n",
    "            ## Now plot the distribution of predicted values\n",
    "            plot_distribution_from_dataloader(dataloader, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Test...\n",
    "num_iterations=101\n",
    "log_dir=\"log\"\n",
    "base_channel_size=16\n",
    "latent_dim=32\n",
    "act_fn=nn.LeakyReLU\n",
    "\n",
    "## The performance seems worse... is it relu/leakyrelu? The added act_fn for the innermost layer, or removing dropout?\n",
    "## It's the final activation layer... hmmmmmmm\n",
    "\n",
    "encoder=EncoderDeep(base_channel_size, latent_dim, act_fn, 0)\n",
    "decoder=DecoderDeep(base_channel_size, latent_dim, act_fn)     \n",
    "\n",
    "#encoder=EncoderSimple(base_channel_size, latent_dim, act_fn)\n",
    "#decoder=DecoderSimple(base_channel_size, latent_dim, act_fn)\n",
    "# print(encoder)\n",
    "# print(decoder)\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "params_to_optimize = [\n",
    "        {'params': encoder.parameters()},\n",
    "        {'params': decoder.parameters()}\n",
    "    ]\n",
    "\n",
    "# lr=1e-2 # For the deep version\n",
    "lr=1e-4\n",
    "weight_decay=0 # 1e-3\n",
    "optimizer = torch.optim.AdamW(params_to_optimize, lr=lr, weight_decay=weight_decay)\n",
    "# optimizer = torch.optim.SGD(params_to_optimize, lr=lr, weight_decay=weight_decay, momentum=0.5)\n",
    "scheduler = None #torch.optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.5)\n",
    "                                            #max_lr=1e-3, total_steps=num_iterations, cycle_momentum=False)\n",
    "\n",
    "## Try a new scheduler\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=5, verbose=True)\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[200,400,600], gamma=0.5, last_epoch=-1, verbose=False)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-2, total_steps=num_iterations, cycle_momentum=False)\n",
    "\n",
    "run_training(num_iterations, log_dir, encoder, decoder, train_loader, optimizer, scheduler)\n",
    "\n",
    "## Look at the post-training parameters\n",
    "print(\"Plotting encoder weights\")\n",
    "plot_weight_distribution(encoder)\n",
    "print(\"Plotting decoder weights\")\n",
    "plot_weight_distribution(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Now take the trained model and try to run some unsupervised learning on it...\n",
    "import pandas as pd \n",
    "\n",
    "## Make a single loader to loop over for ease\n",
    "single_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                            collate_fn=collate,\n",
    "                                            batch_size=1,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=1)\n",
    "\n",
    "encoded_samples = []\n",
    "encoded_images  = []\n",
    "for img in single_loader:\n",
    "    img = img.to(device)\n",
    "    \n",
    "    # Encode image\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded_img  = encoder(img)\n",
    "    # Append to list\n",
    "    encoded_img = encoded_img.flatten().cpu().numpy()\n",
    "    encoded_sample = {f\"Enc. Variable {i}\": enc for i, enc in enumerate(encoded_img)}\n",
    "    # print(type(img.cpu().numpy()))\n",
    "    encoded_sample['nhits'] = np.count_nonzero(img.cpu().numpy())\n",
    "    encoded_samples.append(encoded_sample)\n",
    "    encoded_images .append(encoded_img)\n",
    "encoded_samples = pd.DataFrame(encoded_samples)\n",
    "\n",
    "#labels=pd.DataFrame(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Make a plot of what it looks like\n",
    "plt.scatter(encoded_samples[\"Enc. Variable 0\"], encoded_samples[\"Enc. Variable 1\"],vmin=100, vmax=500, c=encoded_samples[\"nhits\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Now TSNE it up\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "perp=500\n",
    "exag=200\n",
    "print(\"Perplexity =\", perp, \"early exaggeration =\", exag)\n",
    "tsne = TSNE(n_components=2, perplexity=perp, n_iter=1000, early_exaggeration=exag)#, verbose=1, perplexity=60, n_iter=1000, early_exaggeration=20)\n",
    "tsne_results = tsne.fit_transform(encoded_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gr = plt.scatter(list(zip(*tsne_results))[0], list(zip(*tsne_results))[1], s=1, alpha=0.8, vmin=100, vmax=500, c=encoded_samples[\"nhits\"])\n",
    "plt.colorbar(gr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Make a single loader to loop over for ease\n",
    "single_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                            collate_fn=collate,\n",
    "                                            batch_size=1,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=1)\n",
    "\n",
    "encoded_images  = []\n",
    "for img in single_loader:\n",
    "    img = img.to(device)\n",
    "    \n",
    "    # Encode image\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded_img  = encoder(img)\n",
    "    # Append to list\n",
    "    encoded_img = encoded_img.flatten().cpu().numpy()\n",
    "    encoded_images .append(encoded_img)\n",
    "encoded_images = np.vstack(encoded_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Try k-NN algorithm\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Assuming `latent_space` is your latent space representation\n",
    "latent_space = encoded_images #np.array(latent_space)  # Ensure latent_space is a NumPy array\n",
    "\n",
    "# Find the distances to the k-nearest neighbors\n",
    "k = 5  # You can set k equal to min_samples\n",
    "neighbors = NearestNeighbors(n_neighbors=k)\n",
    "neighbors_fit = neighbors.fit(latent_space)\n",
    "distances, indices = neighbors_fit.kneighbors(latent_space)\n",
    "\n",
    "# Sort distances to the k-th nearest neighbor (ascending order)\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:, 1]\n",
    "\n",
    "# Plot the distances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(distances)\n",
    "plt.title('k-NN Distance Plot')\n",
    "plt.xlabel('Points sorted by distance to {}-th nearest neighbor'.format(k))\n",
    "plt.ylabel('Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaled_encoded_images = encoded_images #StandardScaler().fit_transform(encoded_images)\n",
    "\n",
    "plt.scatter(scaled_encoded_images[:, 0], scaled_encoded_images[:, 1], s=1)\n",
    "plt.show()\n",
    "\n",
    "dbscan = DBSCAN(eps=200, min_samples=5)\n",
    "\n",
    "clusters = dbscan.fit(scaled_encoded_images)\n",
    "\n",
    "labels = clusters.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "n_points = [list(labels).count(x) for x in range(n_clusters_)]\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"N. points in clusters:\", n_points)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "print(\"(Out of a total of %d images)\" % len(scaled_encoded_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_labels = set(labels)\n",
    "core_samples_mask = np.zeros_like(labels, dtype=bool)\n",
    "core_samples_mask[dbscan.core_sample_indices_] = True\n",
    "\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = labels == k\n",
    "\n",
    "    xy = scaled_encoded_images[class_member_mask & core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 0],\n",
    "        xy[:, 1],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=14,\n",
    "    )\n",
    "\n",
    "    xy = scaled_encoded_images[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 0],\n",
    "        xy[:, 1],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=0.1,\n",
    "    )\n",
    "\n",
    "plt.title(f\"Estimated number of clusters: {n_clusters_}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now take all of the above, and put it into t-SNE for visualization\n",
    "import pandas as pd \n",
    "\n",
    "encoded_samples = []\n",
    "index=0\n",
    "for img in single_loader:\n",
    "    ## Skip noise\n",
    "    if labels[index] == -1:\n",
    "        index += 1\n",
    "        continue\n",
    "    img = img.to(device)\n",
    "    \n",
    "    # Encode image\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded_img  = encoder(img)\n",
    "    # Append to list\n",
    "    encoded_img = encoded_img.flatten().cpu().numpy()\n",
    "    encoded_sample = {f\"Enc. Variable {i}\": enc for i, enc in enumerate(encoded_img)}\n",
    "    # print(type(img.cpu().numpy()))\n",
    "    encoded_sample['nhits'] = np.count_nonzero(img.cpu().numpy())\n",
    "    encoded_sample['db_cluster'] = labels[index]\n",
    "    encoded_samples.append(encoded_sample)\n",
    "    index+=1\n",
    "\n",
    "encoded_samples = pd.DataFrame(encoded_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Now TSNE it up\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "perp=50\n",
    "exag=50\n",
    "print(\"Perplexity =\", perp, \"early exaggeration =\", exag)\n",
    "tsne = TSNE(n_components=2, perplexity=perp, n_iter=1000, early_exaggeration=exag)#, verbose=1, perplexity=60, n_iter=1000, early_exaggeration=20)\n",
    "tsne_results = tsne.fit_transform(encoded_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Visualise the results\n",
    "plt.scatter(list(zip(*tsne_results))[0], list(zip(*tsne_results))[1], s=4, c=encoded_samples[\"db_cluster\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Function to show examples for each cluster\n",
    "def plot_cluster_examples(raw_images, labels, index, max_images=10): \n",
    "    \n",
    "    plt.figure(figsize=(12,4.5))\n",
    "\n",
    "    ## Get a mask of labels\n",
    "    indices = np.where(np.array(labels) == index)[0]\n",
    "    \n",
    "    ## Grab the first 10 images (if there are 10)\n",
    "    if len(indices) < max_images:\n",
    "        max_images = len(indices)\n",
    "    \n",
    "    ## Plot\n",
    "    for i in range(max_images):\n",
    "        ax = plt.subplot(2,max_images,i+1)\n",
    "        plt.imshow(raw_images[indices[i]], origin='lower')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)            \n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Now pull out a bank of example images for each cluster\n",
    "\n",
    "for index in range(n_clusters_):\n",
    "    print(\"Showing examples for cluster:\", index, \"which has\", n_points[index], \"values\")\n",
    "    plot_cluster_examples(train_dataset, labels, index)\n",
    "\n",
    "print(\"Showing examples for the noise, which has\", n_noise_, \"values\")\n",
    "plot_cluster_examples(train_dataset, labels, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_tools",
   "language": "python",
   "name": "ml_tools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
