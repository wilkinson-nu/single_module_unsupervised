{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import MinkowskiEngine as ME\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import importlib\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torchvision.transforms.v2.functional as F\n",
    "from torch import nn\n",
    "\n",
    "## Jupyter magic\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = [8, 6]\n",
    "mpl.rcParams['font.size'] = 16\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.device(device)\n",
    "import numpy as np\n",
    "SEED=12345\n",
    "_=np.random.seed(SEED)\n",
    "_=torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Includes from my libraries for this project                                                                                                                                           \n",
    "from ME_dataset_libs import make_dense, make_dense_from_tensor, Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FSD_training_analysis import get_models_from_checkpoint\n",
    "\n",
    "## Load in the pre-calculated model weights\n",
    "file_dir = \"/pscratch/sd/c/cwilk\"\n",
    "\n",
    "## This is interesting, but limited so the best performance really is for ~N=20-30. The best silhouette is ~0.25\n",
    "# chk_file = \"state_lat64_hid128_clust25_nchan64_5E-6_1024_PROJ0.5one_CLUST0.5one_ent1E-1_soft1.0_arch24x8silu_poolmax_flat1_grow1_kern7_sep1_onecycle50_bigaugbilinfix0.5_DROP0_WEIGHT_DECAY0.05_10M_DATA1_FSDCCFIX.pth\"\n",
    "chk_file=\"state_lat64_hid128_nchan64_5E-6_1024_PROJ0.5two_arch24x8silu_poolmax_flat1_grow1_kern7_onecycle50_bigaugbilinfix0.5_DROP0_DECAY0_5M_FSDSIMCLR.pth\"\n",
    "\n",
    "encoder, heads, args = get_models_from_checkpoint(file_dir+\"/\"+chk_file)\n",
    "encoder.eval()\n",
    "for h in heads.values(): h.eval()\n",
    "\n",
    "encoder.to(device)\n",
    "for h in heads.values(): h.to(device)\n",
    "\n",
    "print(\"Loaded:\", chk_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Setup the dataloader\n",
    "from FSD_training_analysis import get_dataset\n",
    "import time\n",
    "\n",
    "data_dir = \"/pscratch/sd/c/cwilk/FSD/DATA\"\n",
    "sim_dir = \"/pscratch/sd/c/cwilk/FSD/SIMULATIONv2\"\n",
    "max_data_events=20000\n",
    "max_sim_events=20000\n",
    "\n",
    "start = time.time() \n",
    "sim_dataset, sim_loader = get_dataset(sim_dir, max_sim_events, return_metadata=True)\n",
    "data_dataset, data_loader = get_dataset(data_dir, max_data_events, return_metadata=True)\n",
    "print(\"Time taken to load\", data_dataset.__len__(),\"data and\", sim_dataset.__len__(), \"images:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import FSD_training_analysis\n",
    "importlib.reload(FSD_training_analysis)\n",
    "from FSD_training_analysis import image_loop, reorder_clusters\n",
    "import time\n",
    "start = time.time()\n",
    "## Get the processed vectors of interest from the datasets                                                                                                                                                     \n",
    "data_processed = image_loop(encoder, heads, data_loader, False)\n",
    "sim_processed = image_loop(encoder, heads, sim_loader, False)\n",
    "print(\"Time to process events:\", time.time() - start)\n",
    "\n",
    "start = time.time()\n",
    "## Do some magic to re-order the clusters for presentation purposes                                                                                                                                            \n",
    "reorder_clusters(data_processed, sim_processed)\n",
    "print(\"Time to reorder events:\", time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Play with some GMM options\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def run_gmm_skl(dataset, k, covariance_type='full', max_iter=200):\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    input_vect = scaler.fit_transform(dataset)\n",
    "    \n",
    "    gmm = GaussianMixture(\n",
    "        n_components=k,\n",
    "        n_init=1,\n",
    "        verbose=2,\n",
    "        covariance_type=covariance_type,\n",
    "        max_iter=max_iter\n",
    "    )\n",
    "\n",
    "    gmm.fit(input_vect)\n",
    "\n",
    "    labels = gmm.predict(input_vect)\n",
    "    probs = gmm.predict_proba(input_vect)\n",
    "    aic = gmm.aic(input_vect)\n",
    "    bic = gmm.bic(input_vect)\n",
    "    \n",
    "    print(\"Cluster weights:\", gmm.weights_)\n",
    "    print(\"AIC:\", aic)\n",
    "    print(\"BIC:\", bic)\n",
    "\n",
    "    return labels, aic, bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "def run_faiss_spherical_kmeans(dataset, n_clusters, n_iter=20, verbose=True, seed=123):\n",
    "    # Normalize embeddings (critical for cosine clustering)\n",
    "    X = dataset.astype(np.float32)\n",
    "    X /= np.linalg.norm(X, axis=1, keepdims=True)\n",
    "\n",
    "    N, d = X.shape\n",
    "\n",
    "    # FAISS k-means (spherical via normalization)\n",
    "    kmeans = faiss.Kmeans(\n",
    "        d=d,\n",
    "        k=n_clusters,\n",
    "        niter=n_iter,\n",
    "        verbose=verbose,\n",
    "        seed=seed,\n",
    "        spherical=True  # ensures centroid normalization\n",
    "    )\n",
    "    kmeans.train(X)\n",
    "\n",
    "    # Assign clusters\n",
    "    _, labels = kmeans.index.search(X, 1)\n",
    "    labels = labels.flatten()\n",
    "\n",
    "    # Cluster weights\n",
    "    counts = np.bincount(labels, minlength=n_clusters)\n",
    "    weights = counts / N\n",
    "\n",
    "    # Metrics\n",
    "    labs = np.unique(labels)\n",
    "    metrics = {}\n",
    "\n",
    "    if labs.size < 2 or labs.size >= len(labels):\n",
    "        metrics[\"silhouette\"] = None\n",
    "        metrics[\"calinski_harabasz\"] = None\n",
    "        metrics[\"davies_bouldin\"] = None\n",
    "    else:\n",
    "        metrics[\"silhouette\"] = silhouette_score(X, labels, metric=\"cosine\")\n",
    "        metrics[\"calinski_harabasz\"] = calinski_harabasz_score(X, labels)\n",
    "        metrics[\"davies_bouldin\"] = davies_bouldin_score(X, labels)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Cluster weights:\", weights)\n",
    "        print(\"Silhouette score:\", metrics[\"silhouette\"])\n",
    "        print(\"Calinski-Harabasz =\", metrics[\"calinski_harabasz\"])\n",
    "        print(\"Davies-Bouldin =\", metrics[\"davies_bouldin\"])\n",
    "\n",
    "    return labels, metrics, kmeans.centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spherecluster2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Play with some GMM options\n",
    "import spherecluster\n",
    "from spherecluster import VonMisesFisherMixture\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "def run_vMF(dataset, n_clusters, init=\"random-class\", n_copies=10, verbose=True):\n",
    "\n",
    "    X_norm = dataset / np.linalg.norm(dataset, axis=1, keepdims=True)\n",
    "\n",
    "    ## init: k-means++, spherical-k-means, random, random-class (default), random-orthonormal\n",
    "    ## max_iter: 300\n",
    "    ## n_init: 10\n",
    "    ## n_jobs: 1 (number of CPUs to use)\n",
    "    \n",
    "    vMF = VonMisesFisherMixture(n_clusters=n_clusters, posterior_type='soft', n_init=n_copies, n_jobs=n_copies, verbose=verbose, max_iter=500)\n",
    "    vMF.fit(X_norm)\n",
    "\n",
    "    ## For some reasons labels are floats\n",
    "    labels = vMF.predict(X_norm).astype(int)\n",
    "    weights = vMF.weights_\n",
    "\n",
    "    metrics = {}\n",
    "    metrics[\"silhouette\"] = silhouette_score(X_norm, labels, metric=\"cosine\")\n",
    "    metrics[\"calinski_harabasz\"] = calinski_harabasz_score(X_norm, labels)\n",
    "    metrics[\"davies_bouldin\"] = davies_bouldin_score(X_norm, labels)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Cluster weights:\", weights)\n",
    "        print(\"Silhouette score:\", metrics[\"silhouette\"])\n",
    "        print(\"Calinski-Harabasz =\", metrics[\"calinski_harabasz\"])\n",
    "        print(\"Davies-Bouldin =\", metrics[\"davies_bouldin\"])\n",
    "\n",
    "    return labels, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get a tSNE plot for comparison\n",
    "import ME_analysis_libs\n",
    "importlib.reload(ME_analysis_libs)\n",
    "from ME_analysis_libs import run_tsne_skl\n",
    "ntsne=20000\n",
    "#tsne_results = run_tsne_skl(data_processed['latent'][:ntsne].copy(), \\\n",
    "#                            data_processed['clust_index'][:ntsne].copy(), \\\n",
    "#                            alpha_vect=data_processed['clust_max'][:ntsne].copy(), \\\n",
    "#                            perp=150, exag=20, lr=500)\n",
    "\n",
    "tsne_results = run_tsne_skl(data_processed['latent'][:ntsne].copy(), \\\n",
    "                            np.zeros(ntsne), \\\n",
    "                            perp=150, exag=20, lr=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_k = [10, 20, 30, 40, 50]\n",
    "gmm_labels = []\n",
    "gmm_bic = []\n",
    "gmm_aic = []\n",
    "\n",
    "for k in gmm_k:\n",
    "    print(\"Running k =\", k)\n",
    "    # these_labels, aic, bic = run_gmm_skl(data_processed['latent'], k)\n",
    "    # these_labels, metrics = run_vMF(data_processed['latent'], k)\n",
    "    these_labels, metrics, _ = run_faiss_spherical_kmeans(data_processed['latent'], k)\n",
    "    _ = run_tsne_skl(data_processed['latent'][:ntsne].copy(), \\\n",
    "                     these_labels[:ntsne].copy(), tsne_results=tsne_results)\n",
    "    \n",
    "    gmm_labels.append(these_labels)\n",
    "    #gmm_aic.append(aic)\n",
    "    #gmm_bic.append(bic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 30\n",
    "for init in [\"k-means++\", \"spherical-k-means\", \"random\", \"random-class\", \"random-orthonormal\"]:\n",
    "    print(\"Running init =\", init)\n",
    "    these_labels, metrics = run_vMF(data_processed['latent'], n_clusters, init=init, n_copies=10, verbose=True)\n",
    "    _ = run_tsne_skl(data_processed['latent'][:ntsne].copy(), \\\n",
    "                     these_labels[:ntsne].copy(), tsne_results=tsne_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 30\n",
    "for init in [\"random-orthonormal\"]:\n",
    "    print(\"Running init =\", init)\n",
    "    these_labels, metrics = run_vMF(data_processed['latent'], n_clusters, init=init, n_copies=10, verbose=True)\n",
    "    _ = run_tsne_skl(data_processed['latent'][:ntsne].copy(), \\\n",
    "                     these_labels[:ntsne].copy(), tsne_results=tsne_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_k2 = [60, 70, 80, 90, 100]\n",
    "\n",
    "for k in gmm_k2:\n",
    "    print(\"Running k =\", k)\n",
    "    # these_labels, aic, bic = run_gmm_skl(data_processed['latent'], k)\n",
    "    these_labels = run_vMF(data_processed['latent'], k)\n",
    "    _ = run_tsne_skl(data_processed['latent'][:ntsne].copy(), \\\n",
    "                     these_labels[:ntsne].copy(), tsne_results=tsne_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_tools_ME",
   "language": "python",
   "name": "ml_tools_me"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
