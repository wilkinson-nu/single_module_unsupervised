{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import MinkowskiEngine as ME\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = [8, 6]\n",
    "mpl.rcParams['font.size'] = 16\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "\n",
    "## Tell pytorch we have a GPU if we do\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.device(device)\n",
    "\n",
    "SEED=12345\n",
    "_=np.random.seed(SEED)\n",
    "_=torch.manual_seed(SEED)\n",
    "writer = SummaryWriter(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Use the common dataset loader\n",
    "from ME_dataset_libs import SingleModuleImage2D_MultiHDF5_ME, triple_ME_collate_fn\n",
    "from ME_dataset_libs import make_dense, make_dense_from_tensor, make_dense_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "## This function just pulls an image directly from the file, without going through a pytorch dataloder\n",
    "## You would need to have a file open: f = h5py.File(input_file, 'r')\n",
    "def show_image(i, f):\n",
    "    group = f[str(i)]\n",
    "    data = group['data'][:]\n",
    "    row = group['row'][:]\n",
    "    col = group['col'][:]\n",
    "\n",
    "    ## Use the format that ME requires                                                                                                                                                                         \n",
    "\t## Note that we can't build the sparse tensor here because ME uses some sort of global indexing                                                                                                            \n",
    "\t## And this function is replicated * num_workers                                                                                                                                                           \n",
    "    this_sparse = coo_matrix((data, (row, col)), dtype=np.float32, shape=(800, 256))    \n",
    "    this_image = this_sparse.toarray()\n",
    "\n",
    "    gr = plt.imshow(this_image, origin='lower')\n",
    "    plt.colorbar(gr)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2 as transforms\n",
    "\n",
    "## Need to test a modified splat function in which the threshold is varied randomly in some range\n",
    "## + the probability that a threshold is applied at all should be variable.\n",
    "class BilinearSplatModSpeedy:\n",
    "    def __init__(self, threshold_min=0.04, threshold_max=0.04, p=0.5):\n",
    "        self.threshold_min=threshold_min\n",
    "        self.threshold_max=threshold_max\n",
    "        self.p = p\n",
    "        \n",
    "    def __call__(self, coords, feats):\n",
    "        \n",
    "        feats = np.squeeze(feats)  # Remove single-dimensional entries from shape\n",
    "        \n",
    "        # Floor and ceil coordinates for each point\n",
    "        x0, y0 = np.floor(coords[:, 1]).astype(int), np.floor(coords[:, 0]).astype(int)\n",
    "        x1, y1 = x0 + 1, y0 + 1\n",
    "    \n",
    "        # Calculate the weights for bilinear interpolation\n",
    "        wx1 = coords[:, 1] - x0\n",
    "        wx0 = 1 - wx1\n",
    "        wy1 = coords[:, 0] - y0\n",
    "        wy0 = 1 - wy1\n",
    "\n",
    "        #N = coords.shape[0]\n",
    "        #coords_combined = np.empty((4*N, 2), dtype=int)\n",
    "        #coords_combined[0*N:1*N] = np.stack([y0, x0], axis=-1)\n",
    "        #coords_combined[1*N:2*N] = np.stack([y0, x1], axis=-1)\n",
    "        #coords_combined[2*N:3*N] = np.stack([y1, x0], axis=-1)\n",
    "        #coords_combined[3*N:4*N] = np.stack([y1, x1], axis=-1)\n",
    "\n",
    "        # Coordinates for the four corners\n",
    "        coords00 = np.stack([y0, x0], axis=-1)\n",
    "        coords10 = np.stack([y1, x0], axis=-1)\n",
    "        coords01 = np.stack([y0, x1], axis=-1)\n",
    "        coords11 = np.stack([y1, x1], axis=-1)\n",
    "        \n",
    "        # Calculate interpolated feature values for each of the four corners\n",
    "        f00 = feats * (wx0 * wy0)\n",
    "        f10 = feats * (wx0 * wy1)\n",
    "        f01 = feats * (wx1 * wy0)\n",
    "        f11 = feats * (wx1 * wy1)\n",
    "\n",
    "        #weights = np.stack([wx0*wy0, wx1*wy0, wx0*wy1, wx1*wy1], axis=1)  # (N,4)\n",
    "        #features_combined = (feats[:, None] * weights).reshape(-1)\n",
    "        \n",
    "        # Combine coordinates and features\n",
    "        coords_combined = np.vstack([coords00,coords01,coords10,coords11])\n",
    "        features_combined = np.concatenate([f00, f01, f10, f11])\n",
    "    \n",
    "        # Consolidate features at unique coordinates\n",
    "        #unique_coords, indices = np.unique(coords_combined, axis=0, return_inverse=True)\n",
    "        #summed_feats = np.zeros(len(unique_coords))    \n",
    "        #np.add.at(summed_feats, indices, features_combined)\n",
    "\n",
    "        W = 10000\n",
    "        hash_vals = coords_combined[:,0] * W + coords_combined[:,1]\n",
    "        unique_hashes, inverse = np.unique(hash_vals, return_inverse=True)\n",
    "        summed_feats = np.zeros(len(unique_hashes), dtype=features_combined.dtype)\n",
    "        np.add.at(summed_feats, inverse, features_combined)\n",
    "        unique_coords = np.stack([unique_hashes // W, unique_hashes % W], axis=-1)\n",
    "        \n",
    "        ## Get the threshold\n",
    "        threshold = np.random.uniform(self.threshold_min, self.threshold_max)\n",
    "        \n",
    "        if np.random.rand() < self.p:\n",
    "            # Create a mask for values above the threshold\n",
    "            mask = summed_feats >= threshold\n",
    "    \n",
    "            # Apply the mask to filter features and coordinates\n",
    "            unique_coords = unique_coords[mask]\n",
    "            summed_feats = summed_feats[mask]        \n",
    "        \n",
    "        # Reshape summed_feats to (N, 1)\n",
    "        summed_feats = summed_feats.reshape(-1, 1)\n",
    "        \n",
    "        return unique_coords, summed_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2 as transforms\n",
    "from ME_dataset_libs import get_transform, DoNothing, FirstRegionCrop\n",
    "\n",
    "# Force reload so I can play with changes outside jupyter...\n",
    "import importlib\n",
    "import ME_dataset_libs\n",
    "importlib.reload(ME_dataset_libs)\n",
    "from ME_dataset_libs import MaxRegionCrop, RandomGridDistortion2D, RandomShear2D, RandomRotation2D, RandomHorizontalFlip, \\\n",
    "    RandomBlockZeroImproved, RandomScaleCharge, RandomJitterCharge, ConstantCharge, DoNothing, SemiRandomCrop, ConstantCharge, \\\n",
    "    RandomPixelNoise2D, BilinearSplat, RandomStretch2D, RandomVerticalFlip, RandomInPlaceHorizontalFlip, RandomInPlaceVerticalFlip, \\\n",
    "    SimpleCrop, RandomDropout, JitterCoords, UnlogCharge, RelogCharge, GridJitter, SplitJitterCoords, BilinearSplatMod\n",
    "\n",
    "## 768 is chosen to fit with the current encoder architecture\n",
    "x_max=256\n",
    "y_max=768\n",
    "\n",
    "x_orig=256\n",
    "y_orig=800\n",
    "\n",
    "x_max = x_orig\n",
    "y_max = y_orig\n",
    "\n",
    "mod_transform = transforms.Compose([\n",
    "    \t    RandomBlockZeroImproved([50,100], [5,10], [0,x_orig], [0,y_orig]), ## Does very little\n",
    "            RandomBlockZeroImproved([500,2000], [1,3], [0,x_orig], [0,y_orig]), ## Looks good\n",
    "        \tRandomInPlaceHorizontalFlip(), ## Good\n",
    "            RandomInPlaceVerticalFlip(), ## Good\n",
    "    \t    RandomHorizontalFlip(x_max=x_orig), ## Good\n",
    "            RandomVerticalFlip(y_max=y_orig), ## Good\n",
    "            RandomPixelNoise2D(10),\n",
    "            UnlogCharge(),\n",
    "            GridJitter(),\n",
    "            SplitJitterCoords(10),\n",
    "            RandomShear2D(0.1, 0.1), ## Good\n",
    "            RandomRotation2D(6), ## Good\n",
    "            RandomStretch2D(0.1, 0.1),\n",
    "    \t    RandomGridDistortion2D(100, 5, 2, 25), ## Good\n",
    "    \t    RandomScaleCharge(0.05),\n",
    "        \tRandomJitterCharge(0.05),\n",
    "    \t    BilinearSplatMod(0.3, 0.5, 0.5),\n",
    "            RelogCharge(),\n",
    "       \t    RandomScaleCharge(0.02),\n",
    "        \tRandomJitterCharge(0.02),\n",
    "            SemiRandomCrop(x_max, y_max, 20),\n",
    "            ])\n",
    "\n",
    "aug_transform = mod_transform #\n",
    "# aug_transform = get_transform('fsd', \"vbigaugbilinfixnostretch\")\n",
    "## Load some images into a data loader\n",
    "sim_dir = \"/pscratch/sd/c/cwilk/FSD/SIMULATIONv2\"\n",
    "data_dir = \"/pscratch/sd/c/cwilk/FSD/DATA\"\n",
    "nom_transform = transforms.Compose([\n",
    "            FirstRegionCrop((800, 256), (768, 256)),\n",
    "            # ConstantCharge(),\n",
    "            ])\n",
    "\n",
    "sim_dataset = SingleModuleImage2D_MultiHDF5_ME(sim_dir, nom_transform=nom_transform, aug_transform=aug_transform, max_events=100000)\n",
    "data_dataset = SingleModuleImage2D_MultiHDF5_ME(data_dir, nom_transform=nom_transform, aug_transform=aug_transform, max_events=100000)\n",
    "print(\"Found\", data_dataset.__len__(), \"data events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_ops(A, B):\n",
    "    both = np.array([x for x in A if any((B == x).all(1))])\n",
    "    only_A = np.array([x for x in A if not any((B == x).all(1))])\n",
    "    only_B = np.array([x for x in B if not any((A == x).all(1))])\n",
    "\n",
    "    return both, only_A, only_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def profile_transforms_avg(dataset, transform_block, n_events=10):\n",
    "    times_accum = {t.__class__.__name__: [] for t in transform_block.transforms}\n",
    "\n",
    "    for idx in range(n_events):\n",
    "        _, _, _, _, coords, feats = dataset[idx]\n",
    "        for t in transform_block.transforms:\n",
    "            start = time.perf_counter()\n",
    "            coords, feats = t(coords, feats)\n",
    "            end = time.perf_counter()\n",
    "            times_accum[t.__class__.__name__].append(end - start)\n",
    "\n",
    "    times_avg = {k: np.mean(v) for k, v in times_accum.items()}\n",
    "    return times_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BilinearSplat starts at 8.911 ms averaged over 1000 events\n",
    "times = profile_transforms_avg(data_dataset, mod_transform, n_events=1000)\n",
    "times_sorted = sorted(times.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for name, t in times_sorted:\n",
    "    print(f\"{name:30s}: {t*1000:.3f} ms\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_test(dataset, n=0):\n",
    "\n",
    "    aug1_bcoords, aug1_bfeats, aug2_bcoords, aug2_bfeats, orig_bcoords, orig_bfeats = dataset[n]\n",
    "\n",
    "    print(\"AUG1 #1:\", aug1_bcoords[0], aug1_bfeats[0])\n",
    "    print(\"AUG2 #1:\", aug2_bcoords[0], aug2_bfeats[0])\n",
    "    print(\"ORIG #1:\", orig_bcoords[0], orig_bfeats[0])\n",
    "\n",
    "    print(\"AUG1 SHAPE:\", aug1_bcoords.shape, aug1_bfeats.shape)\n",
    "    print(\"AUG2 SHAPE:\", aug2_bcoords.shape, aug2_bfeats.shape)    \n",
    "    print(\"ORIG SHAPE:\", orig_bcoords.shape, orig_bfeats.shape)\n",
    "\n",
    "    ## Bilinear splat should really have a threshod of ~0.5 to be close to the data distribution...\n",
    "    # print(orig_bfeats.min(), orig_bfeats.max())\n",
    "    both, only_A, only_B = set_ops(orig_bcoords, aug1_bcoords)\n",
    "    print(len(both), len(only_A), len(only_B))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_test(sim_dataset, 69861)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Visualise data\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "def make_aug_comp_plot(dataset, ids=[0], save_name=None):\n",
    "\n",
    "    cmap = cm.turbo.copy()\n",
    "    cmap.set_under(\"#F0F0F0\")\n",
    "\n",
    "    # Visualize the image\n",
    "    n_augs = 5\n",
    "    n_ids = len(ids)\n",
    "    indices = np.arange(n_augs*n_ids)\n",
    "\n",
    "    ## Set up the canvas\n",
    "    plt.figure(figsize=(n_augs*2.1, n_ids*6))\n",
    "\n",
    "    ## To keep track of the subplot\n",
    "    running_n = 0\n",
    "    \n",
    "    ## Loop over images\n",
    "    for i in ids:\n",
    "    \n",
    "        ## The dataset works with pairs, so this is just a bit hacky to get more examples\n",
    "        aug1_bcoords, aug1_bfeats, aug2_bcoords, aug2_bfeats, orig_bcoords, orig_bfeats = dataset[i]\n",
    "        aug3_bcoords, aug3_bfeats, aug4_bcoords, aug4_bfeats, _, _ = dataset[i]\n",
    "\n",
    "        ## Keep the same scale for them all\n",
    "        #orig_max = max(orig_bfeats)\n",
    "        \n",
    "        augs = [make_dense_array(orig_bcoords, orig_bfeats.squeeze(), y_max, x_max),\n",
    "                make_dense_array(aug1_bcoords, aug1_bfeats.squeeze(), y_max, x_max),\n",
    "                make_dense_array(aug2_bcoords, aug2_bfeats.squeeze(), y_max, x_max),\n",
    "                make_dense_array(aug3_bcoords, aug3_bfeats.squeeze(), y_max, x_max),\n",
    "                make_dense_array(aug4_bcoords, aug4_bfeats.squeeze(), y_max, x_max)]\n",
    "\n",
    "        orig_max = augs[0].max()\n",
    "        orig = augs[0]\n",
    "        print(\"orig_min =\", np.min(orig[orig != 0]))\n",
    "\n",
    "        \n",
    "        ## Loop over augmentations\n",
    "        for aug in augs:\n",
    "            ax = plt.subplot(n_ids,n_augs,running_n+1)\n",
    "            # mean_val = np.mean(aug[np.isfinite(aug)]) \n",
    "            mean_val = np.mean(aug[aug != 0])\n",
    "            print(running_n, mean_val)\n",
    "            nonzero_vals = aug[aug > 0]\n",
    "            vmax = np.percentile(nonzero_vals, 80)\n",
    "            ax.imshow(aug, origin='lower', cmap=cmap, vmin=1e-6, vmax=vmax) #, vmax=orig_max)\n",
    "            ax.axis('off')\n",
    "            running_n += 1\n",
    "    plt.tight_layout()\n",
    "    if save_name: plt.savefig(save_name, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def find_biggest_events(dataset, ret_num=1):\n",
    "\n",
    "    size_list = []\n",
    "    \n",
    "    max_event = len(dataset)\n",
    "    for n in range(max_event):\n",
    "        if n%1000==0:\n",
    "            print(\"Processed\", n, \"/\", max_event)\n",
    "        _, _, _, _, _, orig_bfeats = dataset[n]\n",
    "        size_list.append(orig_bfeats.shape[0])\n",
    "\n",
    "    indices = np.argsort(size_list)[::-1]  # reverse for descending order\n",
    "    return indices[:ret_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_list = find_biggest_events(data_dataset, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(big_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualise data\n",
    "def make_aug_diff_plot(dataset, images=[0]):\n",
    "\n",
    "    ## The dataset works with pairs, so this is just a bit hacky to get more examples\n",
    "    aug1_bcoords, aug1_bfeats, aug2_bcoords, aug2_bfeats, orig_bcoords, orig_bfeats = dataset[n]\n",
    "    aug3_bcoords, aug3_bfeats, aug4_bcoords, aug4_bfeats, _, _ = dataset[n]\n",
    "\n",
    "    nom_dense  = make_dense_array(orig_bcoords, orig_bfeats.squeeze(), 800, 256)\n",
    "    aug1_dense = make_dense_array(aug1_bcoords, aug1_bfeats.squeeze(), 800, 256)\n",
    "    aug2_dense = make_dense_array(aug2_bcoords, aug2_bfeats.squeeze(), 800, 256)\n",
    "    aug3_dense = make_dense_array(aug3_bcoords, aug3_bfeats.squeeze(), 800, 256)\n",
    "    aug4_dense = make_dense_array(aug4_bcoords, aug4_bfeats.squeeze(), 800, 256)\n",
    "\n",
    "    diff1 = aug1_dense - nom_dense\n",
    "    diff2 = aug2_dense - nom_dense\n",
    "    diff3 = aug3_dense - nom_dense\n",
    "    diff4 = aug4_dense - nom_dense\n",
    "\n",
    "    augs = [nom_dense, diff1, diff2, diff3, diff4]\n",
    "    \n",
    "    vmax = max(np.max(np.abs(img)) for img in images)\n",
    "    vmin = -vmax  # symmetric around zero\n",
    "    vmin = -1\n",
    "    vmax = 1\n",
    "\n",
    "    ntotal = len(augs)\n",
    "    nimages = len(images)\n",
    "    plt.figure(figsize=(ntotal*1.8,6*nimages))\n",
    "    for i, aug in enumerate(augs, 1):\n",
    "        ax = plt.subplot(nimages, naugs, i)\n",
    "        im = ax.imshow(aug, origin='lower', cmap='seismic', vmin=vmin, vmax=vmax)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        #ax.axis('off')\n",
    "        #plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)  # adjust fraction/pad as needed\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dump a few events!\n",
    "## Interesting numbers: 3, 28, 50, 75, 77, 95, 179, 237, 239, 272, 303, 347, 69861, 73664, 16498\n",
    "#for n in range(400, 450): \n",
    "test_list = [16498, 179, 69861]\n",
    "# test_list = [3, 28, 50, 75, 77, 95, 179, 237, 239, 272, 303, 347, 69861, 73664, 16498]\n",
    "make_aug_comp_plot(data_dataset, test_list, save_name=\"example_augmentations.jpg\")\n",
    "\n",
    "#for n in test_list:\n",
    "    # print(n)\n",
    "    # make_aug_comp_plot(data_dataset, n)\n",
    "    #make_aug_diff_plot(data_dataset, n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_tools_ME",
   "language": "python",
   "name": "ml_tools_me"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
