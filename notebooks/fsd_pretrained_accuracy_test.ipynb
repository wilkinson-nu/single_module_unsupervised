{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import bootstrap\n",
    "import MinkowskiEngine as ME\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.device(device)\n",
    "SEED=12345\n",
    "_=np.random.seed(SEED)\n",
    "_=torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Includes from my libraries for this project                                                                                                                                           \n",
    "from datasets.fsd.augmentations_2d import get_transform\n",
    "from core.data.augmentations_2d import DoNothing\n",
    "from core.data.datasets import paired_2d_dataset_ME, cat_ME_collate_fn\n",
    "from core.analysis.metrics import argmax_consistency, topk_consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.model_utils import get_models_from_checkpoint\n",
    "\n",
    "def calc_accuracy(input_file, nevents):\n",
    "\n",
    "    print(\"Working on:\", input_file)\n",
    "    \n",
    "    encoder, heads, args = get_models_from_checkpoint(input_file)\n",
    "    encoder.eval()\n",
    "    for h in heads.values(): h.eval()\n",
    "\n",
    "    encoder.to(device)\n",
    "    for h in heads.values(): h.to(device)\n",
    "\n",
    "    aug_transform = get_transform('fsd', args.aug_type)\n",
    "    data_dataset = paired_2d_dataset_ME(args.data_dir, nom_transform=DoNothing(), aug_transform=aug_transform, max_events=nevents)\n",
    "\n",
    "    batch_size=1024\n",
    "    train_loader = torch.utils.data.DataLoader(data_dataset,\n",
    "                                               collate_fn=cat_ME_collate_fn,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False, \n",
    "                                               num_workers=8,\n",
    "                                               drop_last=True,\n",
    "                                               pin_memory=False,\n",
    "                                               prefetch_factor=1)\n",
    "\n",
    "    ## Metrics\n",
    "    total_acc = 0\n",
    "    total_top2 = 0\n",
    "    nbatches = 0\n",
    "    \n",
    "    ## Loop over all of the images\n",
    "    for cat_bcoords, cat_bfeats, this_batch_size in train_loader:\n",
    "\n",
    "        nbatches += 1\n",
    "        cat_bcoords = cat_bcoords.to(device, non_blocking=True)\n",
    "        cat_bfeats  = cat_bfeats .to(device)\n",
    "        cat_batch   = ME.SparseTensor(cat_bfeats, cat_bcoords, device=device)\n",
    "\n",
    "        ## Now do the forward pass     \n",
    "        with torch.no_grad(): \n",
    "            encoded_instance_batch, encoded_cluster_batch = encoder(cat_batch, this_batch_size)\n",
    "            clust_batch = heads['clust'](encoded_cluster_batch)\n",
    "            \n",
    "            total_acc += argmax_consistency(clust_batch).item()\n",
    "            total_top2 += topk_consistency(clust_batch, 2)\n",
    "            \n",
    "    print(\"TOTAL ACCURACY top 1:\", total_acc/nbatches, \"; top 2:\", total_top2/nbatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.model_utils import get_models_from_checkpoint\n",
    "\n",
    "## Want to extend this to do two things:\n",
    "## 1 - plot accuracy as a function of max cluster index\n",
    "## 2 - make a smearing matrix showing how often aug1 is in clust X, but clust 2 is in clust Y, normalized such that the sum of each column = 1\n",
    "def calc_accuracy_by_cluster(input_file, nevents):\n",
    "\n",
    "    print(\"Working on:\", input_file)\n",
    "    \n",
    "    encoder, heads, args = get_models_from_checkpoint(input_file)\n",
    "    encoder.eval()\n",
    "    for h in heads.values(): h.eval()\n",
    "\n",
    "    encoder.to(device)\n",
    "    for h in heads.values(): h.to(device)\n",
    "\n",
    "    print(\"Using augs:\", args.aug_type)\n",
    "    aug_transform = get_transform('fsd', args.aug_type)\n",
    "    data_dataset = paired_2d_dataset_ME(args.data_dir, nom_transform=DoNothing(), aug_transform=aug_transform, max_events=nevents)\n",
    "\n",
    "    batch_size=1024\n",
    "    train_loader = torch.utils.data.DataLoader(data_dataset,\n",
    "                                               collate_fn=cat_ME_collate_fn,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False, \n",
    "                                               num_workers=8,\n",
    "                                               drop_last=True,\n",
    "                                               pin_memory=False,\n",
    "                                               prefetch_factor=1)\n",
    "\n",
    "    ## Histogram building\n",
    "    N = args.nclusters\n",
    "    ntotal = np.zeros(N)\n",
    "    ncorrect = np.zeros(N)\n",
    "    smearing = np.zeros((N, N))\n",
    "    \n",
    "    ## Loop over all of the images\n",
    "    for cat_bcoords, cat_bfeats, this_batch_size in train_loader:\n",
    "\n",
    "        cat_bcoords = cat_bcoords.to(device, non_blocking=True)\n",
    "        cat_bfeats  = cat_bfeats .to(device)\n",
    "        cat_batch   = ME.SparseTensor(cat_bfeats, cat_bcoords, device=device)\n",
    "\n",
    "        ## Now do the forward pass     \n",
    "        with torch.no_grad(): \n",
    "            encoded_instance_batch, encoded_cluster_batch = encoder(cat_batch, this_batch_size)\n",
    "            clust_batch = heads['clust'](encoded_cluster_batch)\n",
    "\n",
    "            ## Split batches\n",
    "            clust_batch1 = clust_batch[:this_batch_size//2].detach().cpu().numpy()\n",
    "            clust_batch2 = clust_batch[this_batch_size//2:].detach().cpu().numpy()\n",
    "            \n",
    "            ## Find the selected_cluster for each + keep a running total for the normalization\n",
    "            clust_max1 = np.argmax(clust_batch1, axis=1)\n",
    "            clust_max2 = np.argmax(clust_batch2, axis=1)\n",
    "\n",
    "            counts1 = np.bincount(clust_max1, minlength=N)\n",
    "            counts2 = np.bincount(clust_max2, minlength=N)\n",
    "            ntotal += counts1 #+ counts2\n",
    "            \n",
    "            ## Add to the NxN which will become the covariance\n",
    "            np.add.at(smearing, (clust_max1, clust_max2), 1)\n",
    "            #print(smearing.sum())\n",
    "            #np.add.at(smearing, (clust_max2, clust_max1), 1)\n",
    "               \n",
    "            ## Add to accuracy histogram which is being built\n",
    "            same = (clust_max1 == clust_max2)\n",
    "\n",
    "            counts_same1 =  np.bincount(clust_max1[same], minlength=N)\n",
    "            #counts_same2 =  np.bincount(clust_max2[same], minlength=N)\n",
    "            ncorrect += counts_same1 #+ counts_same2            \n",
    "\n",
    "    ## Divide accuracy histogram by totals to get the average accuracy per selected cluster\n",
    "    smearing_norm = smearing #/ ntotal[np.newaxis, :]\n",
    "    accuracy = np.divide(ncorrect, ntotal) #, out=np.zeros_like(ncorrect, dtype=float), where=ntotal!=0)\n",
    "\n",
    "    ## Return for plotting\n",
    "    return smearing_norm, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = \"/pscratch/sd/c/cwilk\"\n",
    "chk_file = \"state_lat64_hid128_clust30_nchan64_5E-6_1024_PROJ0.5two_CLUST0.5two_ent1E-1_soft1.0_arch24x8silu_poolmax_flat1_grow1_kern7_sep1_onecycle50_baseaugdrop1.0_DROP0_WEIGHT_DECAY0.05_5M_DATA1_FSDCCFIX.pth\"\n",
    "#chk_file = \"state_lat128_hid256_clust30_nchan64_5E-6_1024_PROJ0.5_CLUST0.5two_ent1E-1_soft1.0_arch12x4_poolmax_flat1_grow1_kern7_sep1_onecycle50_vbigaugbilinfixsmooth_1M_DATA1_FSDCCFIX.pth\"\n",
    "nevents = 100000\n",
    "\n",
    "smearing_norm, accuracy = calc_accuracy_by_cluster(file_dir+\"/\"+chk_file, nevents)\n",
    "\n",
    "plt.imshow(smearing_norm, origin='lower', cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='Counts')\n",
    "plt.xlabel('max_arr2')\n",
    "plt.ylabel('max_arr1')\n",
    "plt.title('Smearing Matrix')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(np.arange(len(accuracy)), accuracy)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Count')\n",
    "plt.title('1D Array as Bar Plot')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_tools_ME",
   "language": "python",
   "name": "ml_tools_me"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
