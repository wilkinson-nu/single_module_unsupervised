{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import MinkowskiEngine as ME\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torchvision.transforms.v2.functional as F\n",
    "from torch import nn\n",
    "\n",
    "## Jupyter magic\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = [8, 6]\n",
    "mpl.rcParams['font.size'] = 16\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.device(device)\n",
    "import numpy as np\n",
    "SEED=12345\n",
    "_=np.random.seed(SEED)\n",
    "_=torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Includes from my libraries for this project                                                                                                                                           \n",
    "from ME_dataset_libs import CenterCrop, MaxRegionCrop, ConstantCharge, RandomCrop, RandomPixelNoise2D, FirstRegionCrop\n",
    "from ME_dataset_libs import SingleModuleImage2D_solo_ME, solo_ME_collate_fn, solo_ME_collate_fn_with_meta\n",
    "from ME_dataset_libs import make_dense, make_dense_from_tensor, Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from FSD_training_analysis import get_models_from_checkpoint\n",
    "\n",
    "## Load the pretrained model, set a few other parameters\n",
    "nlatent=256\n",
    "nclusters=20\n",
    "lr=\"1E-5\"\n",
    "batch_size=1024\n",
    "nevts=\"2M\"\n",
    "nsteps=50\n",
    "data_frac=1\n",
    "\n",
    "aug_type=\"bigaugbilin\"\n",
    "nchan=64\n",
    "clust_arch=\"two\"\n",
    "proj_arch=\"logits_\"\n",
    "enc_arch=\"12x4\"\n",
    "enc_arch_pool=\"max\"\n",
    "enc_arch_flatten=1\n",
    "enc_arch_slow_growth=1\n",
    "enc_arch_first_kernel=7\n",
    "enc_arch_sep_heads=1\n",
    "softmax_temp=1.0\n",
    "clust_temp=0.5\n",
    "proj_temp=0.5\n",
    "ent=\"_ent1E-3\"\n",
    "match=\"\"\n",
    "\n",
    "file_dir = \"/pscratch/sd/c/cwilk\"\n",
    "chk_file = \"state_lat\"+str(nlatent)+\"_clust\"+str(nclusters)+match+\"_nchan\"+str(nchan)+\"_\"+lr+\"_\"+str(batch_size)+\\\n",
    "    \"_PROJ\"+str(proj_temp)+proj_arch+\"CLUST\"+str(clust_temp)+clust_arch+ent+\"_soft\"+str(softmax_temp)+\"_arch\"+enc_arch+\"_pool\"+enc_arch_pool+\"_flat\"+str(enc_arch_flatten)+\"_grow\"+str(enc_arch_slow_growth)+\"_kern\"+str(enc_arch_first_kernel)+\"_sep\"+str(enc_arch_sep_heads)+\\\n",
    "    \"_onecycle\"+str(nsteps)+\"_\"+aug_type+\"_\"+nevts+\"_DATA\"+str(data_frac)+\"_FSDCCFIX.pth\"\n",
    "\n",
    "encoder, proj_head, clust_head, args = get_models_from_checkpoint(file_dir+\"/\"+chk_file)\n",
    "encoder.eval()\n",
    "proj_head.eval()\n",
    "clust_head.eval()\n",
    "\n",
    "encoder.to(device)\n",
    "proj_head.to(device)\n",
    "clust_head.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Setup the dataloader\n",
    "from torch.utils.data import ConcatDataset\n",
    "import time\n",
    "start = time.process_time() \n",
    "\n",
    "## Modify the nominal transform\n",
    "nom_transform = transforms.Compose([\n",
    "            FirstRegionCrop((800, 256), (768, 256)),\n",
    "            ])\n",
    "\n",
    "data_dir = \"/pscratch/sd/c/cwilk/FSD/DATA\"\n",
    "sim_dir = \"/pscratch/sd/c/cwilk/FSD/SIMULATION\"\n",
    "single_sim_dataset = SingleModuleImage2D_solo_ME(sim_dir, transform=nom_transform, max_events=250000, return_metadata=True)\n",
    "single_data_dataset = SingleModuleImage2D_solo_ME(data_dir, transform=nom_transform, max_events=250000, return_metadata=True)\n",
    "single_mixed_dataset = ConcatDataset([single_data_dataset, single_sim_dataset])\n",
    "\n",
    "print(\"Time taken to load\", single_data_dataset.__len__(),\"data and\", single_sim_dataset.__len__(), \"images:\", time.process_time() - start)\n",
    "\n",
    "## Randomly chosen batching\n",
    "single_loader = torch.utils.data.DataLoader(single_mixed_dataset,\n",
    "                                            collate_fn=solo_ME_collate_fn_with_meta,\n",
    "                                            batch_size=1024,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Encode the images we'll work with here (can take a while)\n",
    "nhits  = []\n",
    "filenames = []\n",
    "event_ids = []\n",
    "labels = []\n",
    "\n",
    "encoder.eval()\n",
    "\n",
    "## Note that this uses the loader including metadata so it's possible to trace back to the input files\n",
    "for orig_bcoords, orig_bfeats, batch_labels, batch_filenames, batch_eventids in single_loader:\n",
    "\n",
    "    batch_size = len(batch_filenames)\n",
    "    orig_bcoords = orig_bcoords.to(device)\n",
    "    orig_bfeats = orig_bfeats.to(device)\n",
    "    orig_batch = ME.SparseTensor(orig_bfeats, orig_bcoords, device=device)            \n",
    "                                            \n",
    "    ## Now do the forward passes            \n",
    "    with torch.no_grad(): \n",
    "        feature_maps = encoder(orig_batch, batch_size, return_maps=True)\n",
    "\n",
    "        maps = feature_maps[2].cpu()\n",
    "\n",
    "        C = maps.shape[0]  # number of channels\n",
    "        num_to_show = min(C, 16)  # show at most 16 channels\n",
    "\n",
    "        fig, axes = plt.subplots(4, 4, figsize=(8,8))\n",
    "        for i in range(num_to_show):\n",
    "            ax = axes[i//4, i%4]\n",
    "            ax.imshow(maps[i], cmap=\"viridis\", aspect=\"auto\")\n",
    "            ax.set_title(f\"Ch {i}\")\n",
    "            ax.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        break\n",
    "\n",
    "    # nhits += [i.shape[0] for i in orig_batch.decomposed_features] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Function to show examples for each cluster\n",
    "def plot_cluster_examples(dataset, cluster_ids, index, max_images=8): \n",
    "    \n",
    "    plt.figure(figsize=(12,10))\n",
    "\n",
    "    ## Get a mask of cluster_ids\n",
    "    indices = np.where(np.array(cluster_ids) == index)[0]\n",
    "    \n",
    "    ## Grab the first 10 images (if there are 10)\n",
    "    if len(indices) < max_images:\n",
    "        max_images = len(indices)\n",
    "    \n",
    "    ## Plot\n",
    "    for i in range(max_images):\n",
    "        ax = plt.subplot(2,max_images,i+1)\n",
    "        \n",
    "        numpy_coords, numpy_feats, _, _, _ = dataset[indices[i]]\n",
    "\n",
    "        # Create batched coordinates for the SparseTensor input\n",
    "        orig_bcoords  = ME.utils.batched_coordinates([numpy_coords])\n",
    "        orig_bfeats  = torch.from_numpy(np.concatenate([numpy_feats], 0)).float()\n",
    "\n",
    "        orig_bcoords = orig_bcoords.to(device)\n",
    "        orig_bfeats = orig_bfeats.to(device)\n",
    "        orig = ME.SparseTensor(orig_bfeats, orig_bcoords, device=device)\n",
    "            \n",
    "        inputs  = make_dense_from_tensor(orig, 0, 768, 256)\n",
    "        inputs  = inputs .cpu().squeeze().numpy()\n",
    "        \n",
    "        plt.imshow(inputs, origin='lower')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)            \n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_tools_ME",
   "language": "python",
   "name": "ml_tools_me"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
