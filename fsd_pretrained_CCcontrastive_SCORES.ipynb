{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import MinkowskiEngine as ME\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import importlib\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torchvision.transforms.v2.functional as F\n",
    "from torch import nn\n",
    "\n",
    "## Jupyter magic\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = [8, 6]\n",
    "mpl.rcParams['font.size'] = 16\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.device(device)\n",
    "import numpy as np\n",
    "SEED=12345\n",
    "_=np.random.seed(SEED)\n",
    "_=torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Includes from my libraries for this project                                                                                                                                           \n",
    "from ME_dataset_libs import CenterCrop, MaxRegionCrop, ConstantCharge, RandomCrop, RandomPixelNoise2D, FirstRegionCrop\n",
    "from ME_dataset_libs import SingleModuleImage2D_solo_ME, solo_ME_collate_fn, solo_ME_collate_fn_with_meta\n",
    "from ME_dataset_libs import make_dense, make_dense_from_tensor, Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FSD_training_analysis import get_models_from_checkpoint\n",
    "import numpy as np\n",
    "import FSD_training_analysis\n",
    "importlib.reload(FSD_training_analysis)\n",
    "from FSD_training_analysis import image_loop, reorder_clusters\n",
    "\n",
    "def process_images_from_file(input_file):\n",
    "\n",
    "    ## Load in the pre-calculated model weights\n",
    "    file_dir = \"/pscratch/sd/c/cwilk\"\n",
    "    encoder, proj_head, clust_head, args = get_models_from_checkpoint(file_dir+\"/\"+input_file)\n",
    "    encoder.eval()\n",
    "    proj_head.eval()\n",
    "    clust_head.eval()\n",
    "\n",
    "    encoder.to(device)\n",
    "    proj_head.to(device)\n",
    "    clust_head.to(device)\n",
    "\n",
    "    # Modify the nominal transform\n",
    "    nom_transform = FirstRegionCrop((800, 256), (768, 256))\n",
    "    \n",
    "    data_dir = \"/pscratch/sd/c/cwilk/FSD/DATA\"\n",
    "    sim_dir = \"/pscratch/sd/c/cwilk/FSD/SIMULATIONv2\"\n",
    "    max_data_events=100000\n",
    "    max_sim_events=100000\n",
    "    single_sim_dataset = SingleModuleImage2D_solo_ME(sim_dir, transform=nom_transform, max_events=max_sim_events, return_metadata=True)\n",
    "    single_data_dataset = SingleModuleImage2D_solo_ME(data_dir, transform=nom_transform, max_events=max_data_events, return_metadata=True)\n",
    "\n",
    "    ## Randomly chosen batching\n",
    "    data_loader   = torch.utils.data.DataLoader(single_data_dataset,\n",
    "                                                collate_fn=solo_ME_collate_fn_with_meta,\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                num_workers=4)    \n",
    "    sim_loader    = torch.utils.data.DataLoader(single_sim_dataset,\n",
    "                                                collate_fn=solo_ME_collate_fn_with_meta,\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                num_workers=4)\n",
    "\n",
    "    ## Get the processed vectors of interest from the datasets                                                                                                                                                     \n",
    "    data_processed = image_loop(encoder, proj_head, clust_head, data_loader)\n",
    "    sim_processed = image_loop(encoder, proj_head, clust_head, sim_loader)\n",
    "\n",
    "    ## Do some magic to re-order the clusters for presentation purposes                                                                                                                                            \n",
    "    reorder_clusters(data_processed, sim_processed)\n",
    "    \n",
    "    return data_processed, sim_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "def calc_metrics(data_processed):\n",
    "    N, K = data_processed['clust'].shape\n",
    "    silhouette_eucl = silhouette_score(data_processed['clust'], data_processed['clust_index'], metric=\"euclidean\")\n",
    "    print(\"Silhouette (euclidean) =\", silhouette_eucl)\n",
    "    calinski_harabasz = calinski_harabasz_score(data_processed['clust'], data_processed['clust_index'])\n",
    "    print(\"Calinski-Harabasz =\", calinski_harabasz)\n",
    "    davies_bouldin = davies_bouldin_score(data_processed['clust'], data_processed['clust_index'])\n",
    "    print(\"Davies-Bouldin =\", davies_bouldin)\n",
    "    return silhouette_eucl, calinski_harabasz, davies_bouldin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [\"state_lat24_clust25_nchan64_1E-5_1024_PROJ0.5_CLUST0.5two_ent1E-1_soft1.0_arch12x4_poolmax_flat1_grow1_kern7_sep1_onecycle50_bigaugbilin_1M_DATA1_FSDCCFIX.pth\",\n",
    "             \"state_lat32_clust25_nchan64_1E-5_1024_PROJ0.5_CLUST0.5two_ent1E-1_soft1.0_arch12x4_poolmax_flat1_grow1_kern7_sep1_onecycle50_bigaugbilin_1M_DATA1_FSDCCFIX.pth\",\n",
    "             \"state_lat48_clust25_nchan64_1E-5_1024_PROJ0.5_CLUST0.5two_ent1E-1_soft1.0_arch12x4_poolmax_flat1_grow1_kern7_sep1_onecycle50_bigaugbilin_1M_DATA1_FSDCCFIX.pth\",\n",
    "             \"state_lat64_clust25_nchan64_1E-5_1024_PROJ0.5_CLUST0.5two_ent1E-1_soft1.0_arch12x4_poolmax_flat1_grow1_kern7_sep1_onecycle50_bigaugbilin_1M_DATA1_FSDCCFIX.pth\",\n",
    "             \"state_lat128_clust25_nchan64_1E-5_1024_PROJ0.5_CLUST0.5two_ent1E-1_soft1.0_arch12x4_poolmax_flat1_grow1_kern7_sep1_onecycle50_bigaugbilin_1M_DATA1_FSDCCFIX.pth\",\n",
    "             \"state_lat256_clust25_nchan64_1E-5_1024_PROJ0.5_CLUST0.5two_ent1E-1_soft1.0_arch12x4_poolmax_flat1_grow1_kern7_sep1_onecycle50_bigaugbilin_1M_DATA1_FSDCCFIX.pth\"]\n",
    "\n",
    "file_list = [\"state_lat128_clust20_nchan64_1E-5_1024_PROJ0.5_CLUST0.5two_ent0.1_soft1.0_arch12x4_poolmax_flat1_grow1_kern7_sep1_onecycle50_bigaugbilin_1M_DATA1_FSDCCFIX.pth\",\\\n",
    "             \"state_lat128_clust25_nchan64_1E-5_1024_PROJ0.5_CLUST0.5two_ent1E-1_soft1.0_arch12x4_poolmax_flat1_grow1_kern7_sep1_onecycle50_bigaugbilin_1M_DATA1_FSDCCFIX.pth\",\n",
    "             \"state_lat128_clust30_nchan64_1E-5_1024_PROJ0.5_CLUST0.5two_ent1E-1_soft1.0_arch12x4_poolmax_flat1_grow1_kern7_sep1_onecycle50_bigaugbilin_1M_DATA1_FSDCCFIX.pth\"]\n",
    "lat_list = [24, 32, 48, 64, 128, 256]\n",
    "sil_list = []\n",
    "ch_list = []\n",
    "db_list = []\n",
    "\n",
    "## Loop over file\n",
    "for f in file_list:\n",
    "    data_processed, sim_processed = process_images_from_file(f)\n",
    "    silhouette_eucl, calinski_harabasz, davies_bouldin = calc_metrics(data_processed)\n",
    "    sil_list .append(silhouette_eucl)\n",
    "    ch_list  .append(calinski_harabasz)\n",
    "    db_list  .append(davies_bouldin)\n",
    "    print(\"DATA:\", silhouette_eucl, calinski_harabasz, davies_bouldin)\n",
    "    #silhouette_eucl, calinski_harabasz, davies_bouldin = calc_metrics(sim_processed)\n",
    "    #print(\"SIM:\", silhouette_eucl, calinski_harabasz, davies_bouldin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Oh ROOT, how I miss thee\n",
    "import importlib\n",
    "import ME_analysis_libs\n",
    "importlib.reload(ME_analysis_libs)\n",
    "from ME_analysis_libs import parse_binning, plot_metric_sim_data, plot_metric_by_label, plot_metric_by_cluster, plot_metric_data_vs_sim\n",
    "\n",
    "plot_metric_data_vs_sim(data_processed['clust_index'],\n",
    "                        sim_processed['clust_index'], \n",
    "                        sim_processed['labels'],\n",
    "                        xtitle=\"Max. cluster index\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_tools_ME",
   "language": "python",
   "name": "ml_tools_me"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
