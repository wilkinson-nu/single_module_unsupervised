{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import MinkowskiEngine as ME\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torchvision.transforms.v2.functional as F\n",
    "\n",
    "## Jupyter magic\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = [8, 6]\n",
    "mpl.rcParams['font.size'] = 16\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.device(device)\n",
    "import numpy as np\n",
    "SEED=12345\n",
    "_=np.random.seed(SEED)\n",
    "_=torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Includes from my libraries for this project                                                                                                                                           \n",
    "from ME_NN_libs import ContrastiveEncoderME, ContrastiveEncoderShallowME, ContrastiveEncoderFSD, ContrastiveEncoderShallowFSD\n",
    "from ME_dataset_libs import CenterCrop, MaxRegionCrop\n",
    "from ME_dataset_libs import SingleModuleImage2D_solo_ME, solo_ME_collate_fn, solo_ME_collate_fn_with_meta\n",
    "from ME_dataset_libs import make_dense, make_dense_from_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Load the pretrained model, set a few other parameters\n",
    "nchan=16\n",
    "nlatent=128\n",
    "temp=0.5\n",
    "hidden_act_fn=ME.MinkowskiSiLU\n",
    "latent_act_fn=ME.MinkowskiTanh\n",
    "dropout=0\n",
    "lr=\"5e-6\"\n",
    "batch_size=1536\n",
    "aug_type=\"block10x10\"\n",
    "\n",
    "## Define the model\n",
    "encoder=ContrastiveEncoderFSD(nchan, nlatent, hidden_act_fn, latent_act_fn, dropout)\n",
    "\n",
    "## Load in the pre-calculated model weights\n",
    "file_dir = \"/pscratch/sd/c/cwilk\"\n",
    "chk_file = file_dir+\"/state_lat128_nchan16_5e-6_1536_NTXentMerged0.5_onecycle_block10x10_deep_5M0.9_FSD.pth.check50\"\n",
    "\n",
    "checkpoint = torch.load(chk_file, map_location=device)\n",
    "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "encoder.eval()\n",
    "encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Setup the \n",
    "import time\n",
    "start = time.process_time() \n",
    "nom_transform=transforms.Compose([\n",
    "    MaxRegionCrop(),\n",
    "])\n",
    "\n",
    "inDir = file_dir+\"/h5_inputs_v9/\"\n",
    "nevents = 500000\n",
    "\n",
    "train_dataset = SingleModuleImage2D_solo_ME(inDir, transform=nom_transform, max_events=nevents, return_metadata=True)\n",
    "print(\"Time taken to load\", train_dataset.__len__(),\"images:\", time.process_time() - start)\n",
    "\n",
    "## Randomly chosen batching\n",
    "single_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                            collate_fn=solo_ME_collate_fn_with_meta,\n",
    "                                            batch_size=1024,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Encode the images we'll work with here (can take a while)\n",
    "latent = []\n",
    "nhits  = []\n",
    "filenames = []\n",
    "event_ids = []\n",
    "\n",
    "encoder.eval()\n",
    "\n",
    "def extract_latent_numpy(encoded_batch):\n",
    "    if hasattr(encoded_batch, 'decomposed_features'):\n",
    "        return [x.cpu().numpy() for x in encoded_batch.decomposed_features]\n",
    "    else:\n",
    "        return [x[np.newaxis, :] for x in encoded_batch.detach().cpu().numpy()] #encoded_batch.detach().cpu().numpy()\n",
    "\n",
    "## Note that this uses the loader including metadata so it's possible to trace back to the input files\n",
    "for orig_bcoords, orig_bfeats, batch_filenames, batch_eventids in single_loader:\n",
    "\n",
    "    batch_size = len(batch_filenames)\n",
    "    orig_bcoords = orig_bcoords.to(device)\n",
    "    orig_bfeats = orig_bfeats.to(device)\n",
    "    orig_batch = ME.SparseTensor(orig_bfeats, orig_bcoords, device=device)            \n",
    "                                            \n",
    "    ## Now do the forward passes            \n",
    "    with torch.no_grad(): encoded_batch = encoder(orig_batch, batch_size)\n",
    "    \n",
    "    nhits += [i.shape[0] for i in orig_batch.decomposed_features] \n",
    "    latent += extract_latent_numpy(encoded_batch) #x.cpu().numpy() for x in encoded_batch.decomposed_features]\n",
    "    filenames += [i for i in batch_filenames]\n",
    "    event_ids += [i for i in batch_eventids]\n",
    "    \n",
    "lat_nonorm = np.vstack(latent)\n",
    "hit_vect = np.array(nhits)\n",
    "\n",
    "lat_vect = lat_nonorm / np.linalg.norm(lat_nonorm, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a plot of what it looks like (not particularly useful)\n",
    "x_coord = 0\n",
    "y_coord = 1\n",
    "plt.scatter(lat_vect[:,x_coord], lat_vect[:,y_coord], s=1, vmin=100, vmax=500, c=hit_vect)\n",
    "plt.xlabel('Latent #'+str(x_coord))\n",
    "plt.ylabel('Latent #'+str(y_coord))\n",
    "plt.colorbar(label='N. hits')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.manifold import TSNE as cuML_TSNE\n",
    "import cupy as cp\n",
    "\n",
    "## Define a function for running t-SNE using the cuml version\n",
    "def run_tsne_cuml(perp=300, exag=100, input_vect=lat_vect, nhits=hit_vect):\n",
    "    print(\"Running cuML t-SNE with: perplexity =\", perp, \"early exaggeration =\", exag)\n",
    "    \n",
    "    input_vect = cp.asarray(input_vect, dtype=cp.float32)\n",
    "\n",
    "    ## I haven't played with most of cuml's t-SNE parameters\n",
    "    tsne = cuML_TSNE(n_components=2, perplexity=perp, n_iter=1000, early_exaggeration=exag, late_exaggeration=1, metric='cosine') #, n_neighbors=1000)\n",
    "    tsne_results = tsne.fit_transform(input_vect)\n",
    "\n",
    "    tsne_results = cp.asnumpy(tsne_results)  # Convert to NumPy for matplotlib\n",
    "\n",
    "    gr = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], s=0.2, alpha=0.8, vmin=100, vmax=500, c=nhits)\n",
    "    plt.colorbar(gr, label='N.hits')\n",
    "    plt.xlabel('t-SNE #0')\n",
    "    plt.ylabel('t-SNE #1')\n",
    "    plt.show()\n",
    "\n",
    "    return tsne_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Actually run tsne (not always that useful)\n",
    "perp=100\n",
    "exag=20\n",
    "tsne_results = run_tsne_cuml(perp, exag, lat_vect, hit_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.neighbors import NearestNeighbors as cuML_NearestNeighbors\n",
    "\n",
    "## Make a function to show nearest neighbours (not all that useful)\n",
    "def run_knn_cuml(lat_vect, k=5):\n",
    "    # Convert to CuPy array if not already\n",
    "    lat_vect_gpu = cp.asarray(lat_vect, dtype=cp.float32)\n",
    "\n",
    "    # Fit cuML k-NN\n",
    "    neighbors = cuML_NearestNeighbors(n_neighbors=k, metric='cosine')\n",
    "    neighbors.fit(lat_vect_gpu)\n",
    "\n",
    "    distances, indices = neighbors.kneighbors(lat_vect_gpu)\n",
    "\n",
    "    # Convert distances to NumPy for plotting\n",
    "    distances_cpu = cp.asnumpy(distances)\n",
    "\n",
    "    # Sort distances to the k-th nearest neighbor\n",
    "    kth_distances = np.sort(distances_cpu[:, k-1])\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(kth_distances)\n",
    "    plt.title(f'k-NN Distance Plot (k={k})')\n",
    "    plt.xlabel(f'Points sorted by distance to {k}-th nearest neighbor')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.show()\n",
    "\n",
    "    return kth_distances, cp.asnumpy(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Actually run knn\n",
    "run_knn_cuml(lat_vect, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.cluster import DBSCAN\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "## Run DBSCAN using the cuml implementation\n",
    "def run_dbscan_gpu(eps=0.1, min_samples=20, input_vect=None):\n",
    "    if input_vect is None:\n",
    "        raise ValueError(\"input_vect must be provided.\")\n",
    "\n",
    "    print(f\"Running GPU-accelerated DBSCAN with eps={eps}, min_samples={min_samples}\")\n",
    "\n",
    "    # Normalize vectors for cosine similarity (same as CPU version)\n",
    "    input_vect = normalize(input_vect, norm='l2', axis=1)\n",
    "\n",
    "    # Move data to GPU using CuPy\n",
    "    input_vect_gpu = cp.asarray(input_vect)\n",
    "\n",
    "    # Run DBSCAN on GPU\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine', index_type='int64')\n",
    "    labels = dbscan.fit_predict(input_vect_gpu).get()  # Move result back to CPU\n",
    "\n",
    "    # Compute cluster statistics\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise_ = np.sum(labels == -1)\n",
    "    n_points = np.bincount(labels[labels >= 0]) if n_clusters_ > 0 else []\n",
    "\n",
    "    print(f\"Estimated number of clusters: {n_clusters_}\")\n",
    "    print(f\"N. points in clusters: {n_points.tolist()}\")\n",
    "    print(f\"Estimated number of noise points: {n_noise_} (out of {len(input_vect)})\")\n",
    "\n",
    "    return labels, n_clusters_, n_noise_, n_points, dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Actually run an example of dbscan\n",
    "eps=0.06\n",
    "min_samples=100\n",
    "labels, n_clusters_, n_noise_, n_points, dbscan = run_dbscan_gpu(eps, min_samples, input_vect=lat_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Assive colours to each labels\n",
    "unique_labels = set(labels)\n",
    "core_samples_mask = np.zeros_like(labels, dtype=bool)\n",
    "core_samples_mask[dbscan.core_sample_indices_.get()] = True\n",
    "\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = labels == k\n",
    "\n",
    "    xy = lat_vect[class_member_mask & core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 0],\n",
    "        xy[:, 1],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=5,\n",
    "    )\n",
    "\n",
    "    xy = lat_vect[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 0],\n",
    "        xy[:, 1],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=0.1,\n",
    "    )\n",
    "\n",
    "plt.title(f\"Estimated number of clusters: {n_clusters_}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Show the tSNE output (assuming it's been made), with the colours from the clustering\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "cmap = plt.get_cmap('gist_ncar', len(unique_labels))\n",
    "norm = BoundaryNorm(range(len(unique_labels) + 1), cmap.N)\n",
    "plt.scatter(list(zip(*tsne_results))[0], list(zip(*tsne_results))[1], s=1, cmap=cmap, norm=norm, alpha=0.8, c=labels)\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.xlabel('t-SNE #0')\n",
    "plt.ylabel('t-SNE #1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Function to show examples for each cluster\n",
    "def plot_cluster_examples(dataset, labels, index, max_images=10): \n",
    "    \n",
    "    plt.figure(figsize=(12,4.5))\n",
    "\n",
    "    ## Get a mask of labels\n",
    "    indices = np.where(np.array(labels) == index)[0]\n",
    "    \n",
    "    ## Grab the first 10 images (if there are 10)\n",
    "    if len(indices) < max_images:\n",
    "        max_images = len(indices)\n",
    "    \n",
    "    ## Plot\n",
    "    for i in range(max_images):\n",
    "        ax = plt.subplot(2,max_images,i+1)\n",
    "        \n",
    "        numpy_coords, numpy_feats, _, _ = dataset[indices[i]]\n",
    "    \n",
    "        # Create batched coordinates for the SparseTensor input\n",
    "        orig_bcoords  = ME.utils.batched_coordinates([numpy_coords])\n",
    "        orig_bfeats  = torch.from_numpy(np.concatenate([numpy_feats], 0)).float()\n",
    "\n",
    "        orig_bcoords = orig_bcoords.to(device)\n",
    "        orig_bfeats = orig_bfeats.to(device)\n",
    "        orig = ME.SparseTensor(orig_bfeats, orig_bcoords, device=device)\n",
    "            \n",
    "        inputs  = make_dense_from_tensor(orig)\n",
    "        inputs  = inputs .cpu().squeeze().numpy()\n",
    "        \n",
    "        plt.imshow(inputs, origin='lower')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)            \n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now pull out a bank of example images for each cluster\n",
    "for index in range(n_clusters_):\n",
    "    print(\"Showing examples for cluster:\", index, \"which has\", n_points[index], \"values\")\n",
    "    plot_cluster_examples(train_dataset, labels, index, 15)\n",
    "\n",
    "print(\"Showing examples for the noise, which has\", n_noise_, \"values\")\n",
    "plot_cluster_examples(train_dataset, labels, -1, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to show a big block of examples for each cluster\n",
    "## index == None will just give an unclustered set\n",
    "def plot_cluster_bigblock(dataset, labels, index, max_x=10, max_y=10, save_name=None): \n",
    "    \n",
    "    plt.figure(figsize=(max_y*2, max_x*1.8*2))\n",
    "    ## Get a mask of labels\n",
    "    indices = np.arange(max_x*max_y) \n",
    "    if index != None: indices = np.where(np.array(labels) == index)[0]\n",
    "    max_images = min(len(indices), max_x*max_y)\n",
    "    print(len(indices))\n",
    "    \n",
    "    ## Plot\n",
    "    for i in range(max_images):\n",
    "        ax = plt.subplot(max_x,max_y,i+1)\n",
    "        \n",
    "        numpy_coords, numpy_feats, _, _ = dataset[indices[i]]\n",
    "    \n",
    "        # Create batched coordinates for the SparseTensor input\n",
    "        orig_bcoords  = ME.utils.batched_coordinates([numpy_coords])\n",
    "        orig_bfeats  = torch.from_numpy(np.concatenate([numpy_feats], 0)).float()\n",
    "\n",
    "        orig_bcoords = orig_bcoords.to(device)\n",
    "        orig_bfeats = orig_bfeats.to(device)\n",
    "        orig = ME.SparseTensor(orig_bfeats, orig_bcoords, device=device)\n",
    "            \n",
    "        inputs  = make_dense_from_tensor(orig)\n",
    "        inputs  = inputs .cpu().squeeze().numpy()\n",
    "        \n",
    "        plt.imshow(inputs, origin='lower')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)    \n",
    "    plt.tight_layout()\n",
    "    if save_name: plt.savefig(save_name, dpi=150, bbox_inches='tight')\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dump out a large block of images for one cluster\n",
    "plot_cluster_bigblock(train_dataset, labels, 1, 10, 10) #, 'cluster_plots/v9_michel_like.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "## Dump out a file including the filenames and indices for the clustered images (for going back to the original files)\n",
    "def dump_cluster_indices(index_label, cluster_labels, filenames, event_ids):\n",
    "\n",
    "    # Inputs\n",
    "    indices = np.where(cluster_labels == index_label)[0]\n",
    "\n",
    "    selected_filenames = np.array(filenames)[indices]\n",
    "    selected_event_ids = np.array(event_ids)[indices]\n",
    "\n",
    "    # Group by filename\n",
    "    grouped = defaultdict(list)\n",
    "    for fname, eid in zip(selected_filenames, selected_event_ids):\n",
    "        grouped[fname].append(int(eid))  # ensure JSON serializability\n",
    "\n",
    "    # Save to JSON\n",
    "    output_file = f'cluster_{index_label}_events.json'\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(grouped, f, indent=2)\n",
    "\n",
    "    print(f\"Saved grouped event list for cluster {index_label} to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dump_cluster_indices(1, labels, filenames, event_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_tools_ME",
   "language": "python",
   "name": "ml_tools_me"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
