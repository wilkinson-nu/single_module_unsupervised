{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import MinkowskiEngine as ME\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torchvision.transforms.v2.functional as F\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.device(device)\n",
    "SEED=12345\n",
    "_=np.random.seed(SEED)\n",
    "_=torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Includes from my libraries for this project                                                                                                                                           \n",
    "from ME_dataset_libs import get_transform, DoNothing\n",
    "from ME_dataset_libs import SingleModuleImage2D_MultiHDF5_ME, cat_ME_collate_fn\n",
    "from ME_analysis_libs import argmax_consistency, topk_consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FSD_training_analysis import get_models_from_checkpoint\n",
    "\n",
    "def calc_accuracy(input_file, nevents):\n",
    "\n",
    "    print(\"Working on:\", input_file)\n",
    "    \n",
    "    encoder, proj_head, clust_head, args = get_models_from_checkpoint(input_file)\n",
    "    encoder.eval()\n",
    "    proj_head.eval()\n",
    "    clust_head.eval()\n",
    "\n",
    "    encoder.to(device)\n",
    "    proj_head.to(device)\n",
    "    clust_head.to(device)\n",
    "\n",
    "    aug_transform = get_transform('fsd', args.aug_type)\n",
    "    data_dataset = SingleModuleImage2D_MultiHDF5_ME(args.data_dir, nom_transform=DoNothing(), aug_transform=aug_transform, max_events=nevents)\n",
    "\n",
    "    batch_size=1024\n",
    "    train_loader = torch.utils.data.DataLoader(data_dataset,\n",
    "                                               collate_fn=cat_ME_collate_fn,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False, \n",
    "                                               num_workers=8,\n",
    "                                               drop_last=True,\n",
    "                                               pin_memory=False,\n",
    "                                               prefetch_factor=1)\n",
    "\n",
    "    ## Metrics\n",
    "    total_acc = 0\n",
    "    total_top2 = 0\n",
    "    nbatches = 0\n",
    "    \n",
    "    ## Loop over all of the images\n",
    "    for cat_bcoords, cat_bfeats, this_batch_size in train_loader:\n",
    "\n",
    "        nbatches += 1\n",
    "        cat_bcoords = cat_bcoords.to(device, non_blocking=True)\n",
    "        cat_bfeats  = cat_bfeats .to(device)\n",
    "        cat_batch   = ME.SparseTensor(cat_bfeats, cat_bcoords, device=device)\n",
    "\n",
    "        ## Now do the forward pass     \n",
    "        with torch.no_grad(): \n",
    "            encoded_instance_batch, encoded_cluster_batch = encoder(cat_batch, this_batch_size)\n",
    "            clust_batch = clust_head(encoded_cluster_batch)\n",
    "            \n",
    "            total_acc += argmax_consistency(clust_batch).item()\n",
    "            total_top2 += topk_consistency(clust_batch, 2)\n",
    "            \n",
    "    print(\"TOTAL ACCURACY top 1:\", total_acc/nbatches, \"; top 2:\", total_top2/nbatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FSD_training_analysis import get_models_from_checkpoint\n",
    "\n",
    "## Want to extend this to do two things:\n",
    "## 1 - plot accuracy as a function of max cluster index\n",
    "## 2 - make a smearing matrix showing how often aug1 is in clust X, but clust 2 is in clust Y, normalized such that the sum of each column = 1\n",
    "def calc_accuracy_by_cluster(input_file, nevents):\n",
    "\n",
    "    print(\"Working on:\", input_file)\n",
    "    \n",
    "    encoder, proj_head, clust_head, args = get_models_from_checkpoint(input_file)\n",
    "    encoder.eval()\n",
    "    proj_head.eval()\n",
    "    clust_head.eval()\n",
    "\n",
    "    encoder.to(device)\n",
    "    proj_head.to(device)\n",
    "    clust_head.to(device)\n",
    "\n",
    "    print(\"Using augs:\", args.aug_type)\n",
    "    aug_transform = get_transform('fsd', args.aug_type)\n",
    "    data_dataset = SingleModuleImage2D_MultiHDF5_ME(args.data_dir, nom_transform=DoNothing(), aug_transform=aug_transform, max_events=nevents)\n",
    "\n",
    "    batch_size=1024\n",
    "    train_loader = torch.utils.data.DataLoader(data_dataset,\n",
    "                                               collate_fn=cat_ME_collate_fn,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False, \n",
    "                                               num_workers=8,\n",
    "                                               drop_last=True,\n",
    "                                               pin_memory=False,\n",
    "                                               prefetch_factor=1)\n",
    "\n",
    "    ## Histogram building\n",
    "    N = args.nclusters\n",
    "    ntotal = np.zeros(N)\n",
    "    ncorrect = np.zeros(N)\n",
    "    smearing = np.zeros((N, N))\n",
    "    \n",
    "    ## Loop over all of the images\n",
    "    for cat_bcoords, cat_bfeats, this_batch_size in train_loader:\n",
    "\n",
    "        cat_bcoords = cat_bcoords.to(device, non_blocking=True)\n",
    "        cat_bfeats  = cat_bfeats .to(device)\n",
    "        cat_batch   = ME.SparseTensor(cat_bfeats, cat_bcoords, device=device)\n",
    "\n",
    "        ## Now do the forward pass     \n",
    "        with torch.no_grad(): \n",
    "            encoded_instance_batch, encoded_cluster_batch = encoder(cat_batch, this_batch_size)\n",
    "            clust_batch = clust_head(encoded_cluster_batch)\n",
    "\n",
    "            ## Split batches\n",
    "            clust_batch1 = clust_batch[:this_batch_size//2].detach().cpu().numpy()\n",
    "            clust_batch2 = clust_batch[this_batch_size//2:].detach().cpu().numpy()\n",
    "            \n",
    "            ## Find the selected_cluster for each + keep a running total for the normalization\n",
    "            clust_max1 = np.argmax(clust_batch1, axis=1)\n",
    "            clust_max2 = np.argmax(clust_batch2, axis=1)\n",
    "\n",
    "            counts1 = np.bincount(clust_max1, minlength=N)\n",
    "            counts2 = np.bincount(clust_max2, minlength=N)\n",
    "            ntotal += counts1 #+ counts2\n",
    "            \n",
    "            ## Add to the NxN which will become the covariance\n",
    "            np.add.at(smearing, (clust_max1, clust_max2), 1)\n",
    "            #print(smearing.sum())\n",
    "            #np.add.at(smearing, (clust_max2, clust_max1), 1)\n",
    "               \n",
    "            ## Add to accuracy histogram which is being built\n",
    "            same = (clust_max1 == clust_max2)\n",
    "\n",
    "            counts_same1 =  np.bincount(clust_max1[same], minlength=N)\n",
    "            #counts_same2 =  np.bincount(clust_max2[same], minlength=N)\n",
    "            ncorrect += counts_same1 #+ counts_same2            \n",
    "\n",
    "    ## Divide accuracy histogram by totals to get the average accuracy per selected cluster\n",
    "    smearing_norm = smearing #/ ntotal[np.newaxis, :]\n",
    "    accuracy = np.divide(ncorrect, ntotal) #, out=np.zeros_like(ncorrect, dtype=float), where=ntotal!=0)\n",
    "\n",
    "    ## Return for plotting\n",
    "    return smearing_norm, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = \"/pscratch/sd/c/cwilk\"\n",
    "chk_file = \"state_lat128_hid256_clust30_nchan64_5E-6_1024_PROJ0.5_CLUST0.5two_ent1E-1_soft1.0_arch12x4_poolmax_flat1_grow1_kern7_sep1_onecycle50_vbigaugbilinfixsmooth_1M_DATA1_FSDCCFIX.pth\"\n",
    "nevents = 100000\n",
    "\n",
    "smearing_norm, accuracy = calc_accuracy_by_cluster(file_dir+\"/\"+chk_file, nevents)\n",
    "\n",
    "plt.imshow(smearing_norm, origin='lower', cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='Counts')\n",
    "plt.xlabel('max_arr2')\n",
    "plt.ylabel('max_arr1')\n",
    "plt.title('Smearing Matrix')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(np.arange(len(accuracy)), accuracy)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Count')\n",
    "plt.title('1D Array as Bar Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [\"state_lat24_clust25_nchan64_1E-5_1024_PROJ0.5_CLUST0.5two_ent1E-1_soft1.0_arch12x4_poolmax_flat1_grow1_kern7_sep1_onecycle50_bigaugbilin_1M_DATA1_FSDCCFIX.pth\",\n",
    "             \"state_lat32_clust25_nchan64_1E-5_1024_PROJ0.5_CLUST0.5two_ent1E-1_soft1.0_arch12x4_poolmax_flat1_grow1_kern7_sep1_onecycle50_bigaugbilin_1M_DATA1_FSDCCFIX.pth\",\n",
    "             \"state_lat48_clust25_nchan64_1E-5_1024_PROJ0.5_CLUST0.5two_ent1E-1_soft1.0_arch12x4_poolmax_flat1_grow1_kern7_sep1_onecycle50_bigaugbilin_1M_DATA1_FSDCCFIX.pth\",\n",
    "             \"state_lat64_clust25_nchan64_1E-5_1024_PROJ0.5_CLUST0.5two_ent1E-1_soft1.0_arch12x4_poolmax_flat1_grow1_kern7_sep1_onecycle50_bigaugbilin_1M_DATA1_FSDCCFIX.pth\",\n",
    "             \"state_lat128_clust25_nchan64_1E-5_1024_PROJ0.5_CLUST0.5two_ent1E-1_soft1.0_arch12x4_poolmax_flat1_grow1_kern7_sep1_onecycle50_bigaugbilin_1M_DATA1_FSDCCFIX.pth\",\n",
    "             \"state_lat256_clust25_nchan64_1E-5_1024_PROJ0.5_CLUST0.5two_ent1E-1_soft1.0_arch12x4_poolmax_flat1_grow1_kern7_sep1_onecycle50_bigaugbilin_1M_DATA1_FSDCCFIX.pth\"]\n",
    "\n",
    "for f in file_list:\n",
    "    calc_accuracy(file_dir+\"/\"+f, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = \"/pscratch/sd/c/cwilk\"\n",
    "aug=\"newbig\"\n",
    "for clust_temp in [0.25, 0.5, 0.75]:\n",
    "    for proj_temp in [0.25, 0.5, 0.75]:\n",
    "        print(\"CLUST_TEMP =\", clust_temp, \"; PROJ_TEMP =\", proj_temp)\n",
    "        chk_file = \"state_lat128_clust30_nchan64_5E-5_1024_PROJ\"+str(proj_temp)+\"logits_CLUST\"+str(clust_temp)+\"one_soft1.0_arch12x4_poolmax_flat1_grow1_kern7_sep1_onecycle50_\"+aug+\"_5M_DATA1_FSDCC.pth\"\n",
    "        calc_accuracy(file_dir+\"/\"+chk_file, 200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = \"/pscratch/sd/c/cwilk\"\n",
    "\n",
    "for ent in [\"_ENT0.01\", \"_ENT0.1\", \"_ENT0.5\", \"\"]:\n",
    "    chk_file = \"state_lat128_clust30\"+ent+\"_nchan64_5E-5_1024_PROJ0.5logits_CLUST0.5one_soft1.0_arch12x4_poolmax_flat1_grow1_kern7_sep1_onecycle50_newbig_5M_DATA1_FSDCC.pth\"\n",
    "    calc_accuracy(file_dir+\"/\"+chk_file, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = \"/pscratch/sd/c/cwilk\"\n",
    "chk_file = \"state_lat128_clust30_MATCH1.0_nchan64_5E-5_1024_PROJ0.5logits_CLUST0.5one_soft1.0_arch12x4_poolmax_flat1_grow1_kern7_sep1_onecycle50_newbig2_2M_DATA1_FSDCC.pth\"\n",
    "calc_accuracy(file_dir+\"/\"+chk_file, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_tools_ME",
   "language": "python",
   "name": "ml_tools_me"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
