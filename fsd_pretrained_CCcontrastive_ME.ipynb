{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import MinkowskiEngine as ME\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import importlib\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torchvision.transforms.v2.functional as F\n",
    "from torch import nn\n",
    "\n",
    "## Jupyter magic\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = [8, 6]\n",
    "mpl.rcParams['font.size'] = 16\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.device(device)\n",
    "import numpy as np\n",
    "SEED=12345\n",
    "_=np.random.seed(SEED)\n",
    "_=torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Includes from my libraries for this project                                                                                                                                           \n",
    "from ME_dataset_libs import CenterCrop, MaxRegionCrop, ConstantCharge, RandomCrop, RandomPixelNoise2D, FirstRegionCrop\n",
    "from ME_dataset_libs import SingleModuleImage2D_solo_ME, solo_ME_collate_fn, solo_ME_collate_fn_with_meta\n",
    "from ME_dataset_libs import make_dense, make_dense_from_tensor, Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FSD_training_analysis import get_models_from_checkpoint\n",
    "nlatent=128\n",
    "nclusters=20\n",
    "lr=\"5E-6\"\n",
    "batch_size=1024\n",
    "nevts=\"5M\"\n",
    "nsteps=50\n",
    "data_frac=1\n",
    "nhidden=\"_hid256\"\n",
    "\n",
    "aug_type=\"newbaseaug\"\n",
    "aug_prob=\"1\"\n",
    "nchan=64\n",
    "clust_arch=\"two\"\n",
    "proj_arch=\"two\"\n",
    "enc_arch=\"d4silu\"\n",
    "enc_arch_pool=\"max\"\n",
    "enc_arch_flatten=0\n",
    "enc_arch_slow_growth=1\n",
    "enc_arch_first_kernel=7\n",
    "enc_arch_sep_heads=0\n",
    "softmax_temp=1.0\n",
    "clust_temp=0.5\n",
    "proj_temp=0.5\n",
    "ent=\"_ent1E-1\"\n",
    "data_string=\"DATA\"+str(data_frac)\n",
    "\n",
    "## Load in the pre-calculated model weights\n",
    "file_dir = \"/pscratch/sd/c/cwilk\"\n",
    "chk_file = \"state_lat\"+str(nlatent)+nhidden+\"_clust\"+str(nclusters)+\"_nchan\"+str(nchan)+\"_\"+lr+\"_\"+str(batch_size)+\\\n",
    "    \"_PROJ\"+str(proj_temp)+proj_arch+\"_CLUST\"+str(clust_temp)+clust_arch+ent+\"_soft\"+str(softmax_temp)+\\\n",
    "    \"_arch\"+enc_arch+\"_pool\"+enc_arch_pool+\"_flat\"+str(enc_arch_flatten)+\"_grow\"+str(enc_arch_slow_growth)+\\\n",
    "    \"_kern\"+str(enc_arch_first_kernel)+\"_sep\"+str(enc_arch_sep_heads)+\\\n",
    "    \"_onecycle\"+str(nsteps)+\"_\"+aug_type+aug_prob+\"_DROP0_WEIGHT_DECAY0_\"+nevts+\"_\"+data_string+\"_FSDCCFIX.pth\"\n",
    "\n",
    "encoder, heads, args = get_models_from_checkpoint(file_dir+\"/\"+chk_file)\n",
    "encoder.eval()\n",
    "for h in heads.values(): h.eval()\n",
    "\n",
    "encoder.to(device)\n",
    "for h in heads.values(): h.to(device)\n",
    "\n",
    "print(\"Loaded:\", chk_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Setup the dataloader\n",
    "from ME_dataset_libs import FirstRegionCrop\n",
    "from torch.utils.data import ConcatDataset\n",
    "import time\n",
    "start = time.time() \n",
    "\n",
    "## Modify the nominal transform\n",
    "nom_transform = transforms.Compose([\n",
    "            FirstRegionCrop((800, 256), (768, 256)),\n",
    "            # ConstantCharge(),\n",
    "            ])\n",
    "\n",
    "data_dir = \"/pscratch/sd/c/cwilk/FSD/DATA\" #_MIN200_v3\"\n",
    "# sim_dir = \"/pscratch/sd/c/cwilk/FSD/DATAv6\"\n",
    "sim_dir = \"/pscratch/sd/c/cwilk/FSD/SIMULATIONv2\" #_MIN200_v2\"\n",
    "max_data_events=50000\n",
    "max_sim_events=50000\n",
    "single_sim_dataset = SingleModuleImage2D_solo_ME(sim_dir, transform=nom_transform, max_events=max_sim_events, return_metadata=True)\n",
    "single_data_dataset = SingleModuleImage2D_solo_ME(data_dir, transform=nom_transform, max_events=max_data_events, return_metadata=True)\n",
    "single_mixed_dataset = ConcatDataset([single_data_dataset, single_sim_dataset])\n",
    "\n",
    "print(\"Time taken to load\", single_data_dataset.__len__(),\"data and\", single_sim_dataset.__len__(), \"images:\", time.time() - start)\n",
    "\n",
    "## Randomly chosen batching\n",
    "single_loader = torch.utils.data.DataLoader(single_mixed_dataset,\n",
    "                                            collate_fn=solo_ME_collate_fn_with_meta,\n",
    "                                            batch_size=1024,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=4)\n",
    "data_loader   = torch.utils.data.DataLoader(single_data_dataset,\n",
    "                                            collate_fn=solo_ME_collate_fn_with_meta,\n",
    "                                            batch_size=1024,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=4)\n",
    "\n",
    "sim_loader    = torch.utils.data.DataLoader(single_sim_dataset,\n",
    "                                            collate_fn=solo_ME_collate_fn_with_meta,\n",
    "                                            batch_size=1024,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import FSD_training_analysis\n",
    "importlib.reload(FSD_training_analysis)\n",
    "from FSD_training_analysis import image_loop, reorder_clusters\n",
    "import time\n",
    "start = time.time()\n",
    "## Get the processed vectors of interest from the datasets                                                                                                                                                     \n",
    "data_processed = image_loop(encoder, heads, data_loader, False)\n",
    "sim_processed = image_loop(encoder, heads, sim_loader, False)\n",
    "print(\"Time to process events:\", time.time() - start)\n",
    "\n",
    "start = time.time()\n",
    "## Do some magic to re-order the clusters for presentation purposes                                                                                                                                            \n",
    "reorder_clusters(data_processed, sim_processed)\n",
    "print(\"Time to reorder events:\", time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force reload so I can play with changes outside jupyter...\n",
    "import importlib\n",
    "import ME_analysis_libs\n",
    "importlib.reload(ME_analysis_libs)\n",
    "from ME_analysis_libs import compute_cluster_overlap, plot_overlap_matrix\n",
    "\n",
    "overlap_matrix = compute_cluster_overlap(data_processed['clust'], 3)\n",
    "plot_overlap_matrix(overlap_matrix, max_val=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Oh ROOT, how I miss thee\n",
    "import importlib\n",
    "import ME_analysis_libs\n",
    "importlib.reload(ME_analysis_libs)\n",
    "from ME_analysis_libs import parse_binning, plot_metric_pass_fail, plot_metric_by_label, plot_metric_by_cluster, plot_metric_data_vs_sim, plot_metric_by_confidence\n",
    "\n",
    "plot_metric_data_vs_sim(data_processed['clust_index'],\n",
    "                        sim_processed['clust_index'], \n",
    "                        sim_processed['labels'],\n",
    "                        xtitle=\"Max. cluster index\")\n",
    "\n",
    "#plot_metric_by_confidence(data_processed['clust_index'], \n",
    "#                          data_processed['clust_max'],\n",
    "#                          xtitle=\"Max. cluster index\")\n",
    "\n",
    "#min_hits=250\n",
    "#data_mask = (data_processed['nhits']>min_hits)\n",
    "#plot_metric_pass_fail(data_processed['clust_index'], \n",
    "#                      data_mask, xtitle=\"Max. cluster index\")\n",
    "\n",
    "#sim_mask = (sim_processed['nhits']>min_hits)\n",
    "#plot_metric_pass_fail(sim_processed['clust_index'], \n",
    "#                      sim_mask, xtitle=\"Max. cluster index\")\n",
    "\n",
    "# plot_metric_data_vs_sim(data_processed['xrange'], sim_processed['xrange'], sim_processed['labels'], nbinsx=50, xtitle=\"x-range\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Look at the distribution of values\n",
    "plot_metric_data_vs_sim(data_processed['clust_max'], sim_processed['clust_max'], sim_processed['labels'], nbinsx=50, xtitle=\"Top prob.\")\n",
    "plot_metric_by_cluster(data_processed['clust_max'], data_processed['clust_index'], nbinsx=50, xtitle=\"Top prob.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Look at the number of significant clusters\n",
    "plot_metric_data_vs_sim(data_processed['nhits'], sim_processed['nhits'], sim_processed['labels'], nbinsx=70, x_max=1400, xtitle=\"N. hits\")\n",
    "plot_metric_data_vs_sim(data_processed['maxQ'], sim_processed['maxQ'], sim_processed['labels'], nbinsx=100, x_min=1.5, x_max=2.5, xtitle=\"Max. Q\")\n",
    "plot_metric_data_vs_sim(data_processed['sumQ'], sim_processed['sumQ'], sim_processed['labels'], nbinsx=70, x_max=1400, xtitle=\"Sum Q\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "import matplotlib.colors as mcolors\n",
    "from cuml.manifold import TSNE as cuML_TSNE\n",
    "import cupy as cp\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, normalize\n",
    "from cuml.preprocessing import StandardScaler as cuMLScaler\n",
    "def run_tsne_cuml_ALT(input_vect=None, zvect=None, alpha_vect=None, perp=30, exag=6, lr=2000.0, n_neighbors=50, tsne_results=None, ztitle=\"Cluster ID\", save_name=None, norm=True):\n",
    "\n",
    "    print(\"Running cuML t-SNE with: perplexity =\", perp, \"early exaggeration =\", exag)\n",
    "\n",
    "    input_vect = cp.asarray(input_vect, dtype=cp.float32)\n",
    "\n",
    "    if norm:\n",
    "        norms = cp.linalg.norm(input_vect, axis=1, keepdims=True)\n",
    "        input_vect = input_vect / (norms + 1e-10)\n",
    "\n",
    "    n_neighbors = 3*perp\n",
    "    if n_neighbors > 1024: n_neighbors = 1024\n",
    "    \n",
    "    ## I haven't played with most of cuml's t-SNE parameters\n",
    "    #tsne = cuML_TSNE(n_components=2, perplexity=perp, n_iter=5000, \\\n",
    "    #                 early_exaggeration=exag, learning_rate=lr, exaggeration_iter=250, \\\n",
    "    #                 learning_rate_method=None, square_distances=False, init='random', late_exaggeration=1, \\\n",
    "    #                 metric='cosine', method='fft', verbose=True, n_neighbors=n_neighbors)\n",
    "    tsne = cuML_TSNE(n_components=2, perplexity=perp, n_iter=1000, \\\n",
    "                     learning_rate_method=None, early_exaggeration=exag, learning_rate=lr, method='exact', #barnes_hut',\\\n",
    "                     metric='cosine', square_distances=False, init='random', n_neighbors=n_neighbors, verbose=True)\n",
    "    #tsne = cuML_TSNE(n_components=2, perplexity=perp, n_iter=5000, \\\n",
    "    #                 early_exaggeration=exag, learning_rate=lr, method='barnes_hut',\\\n",
    "    #                 metric='cosine', square_distances=False, verbose=True)\n",
    "    \n",
    "    if tsne_results is None:\n",
    "        tsne_results = tsne.fit_transform(input_vect)\n",
    "        scaler = cuMLScaler()\n",
    "        tsne_results = scaler.fit_transform(tsne_results)  # tsne_results still on GPU\n",
    "        tsne_results = cp.asnumpy(tsne_results)\n",
    "        \n",
    "    unique_labels = np.unique(zvect)\n",
    "    n_clusters = len(unique_labels)\n",
    "\n",
    "    # Use a qualitative colormap with enough colors\n",
    "    all_colors = (\n",
    "        plt.cm.tab20.colors +\n",
    "        plt.cm.tab20b.colors +\n",
    "        plt.cm.tab20c.colors +\n",
    "        plt.cm.tab10.colors\n",
    "    )\n",
    "    cmap = mcolors.ListedColormap(all_colors[:n_clusters])\n",
    "\n",
    "    alpha_vect = alpha_vect**3\n",
    "    rgb_colors = np.array([cmap(i % n_clusters)[:3] for i in zvect])\n",
    "\n",
    "    # add per-point alpha (density-based)\n",
    "    rgb_colors = np.concatenate([rgb_colors, alpha_vect[:, None]], axis=1)\n",
    "    norm = mcolors.BoundaryNorm(boundaries=np.arange(n_clusters + 1), ncolors=n_clusters)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(tsne_results[:, 0], tsne_results[:, 1], s=0.5, c=rgb_colors)\n",
    "\n",
    "    cbar = fig.colorbar(\n",
    "        plt.cm.ScalarMappable(norm=norm, cmap=cmap),ax=ax\n",
    "    )\n",
    "    cbar.set_label(ztitle)\n",
    "    #plt.colorbar(gr, label=ztitle)\n",
    "    plt.xlabel('t-SNE #0')\n",
    "    plt.ylabel('t-SNE #1')\n",
    "    if save_name: plt.savefig(save_name, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    return tsne_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ME_analysis_libs\n",
    "importlib.reload(ME_analysis_libs)\n",
    "from ME_analysis_libs import run_tsne_cuml, run_tsne_skl\n",
    "import time\n",
    "\n",
    "## Actually run tsne (not always that useful)\n",
    "perp=150\n",
    "exag=16\n",
    "lr=4000.0\n",
    "\n",
    "data_mask = (data_processed['nhits']>250)\n",
    "sim_mask = (sim_processed['nhits']>250)\n",
    "\n",
    "ntsne=5000\n",
    "data_latent_subset = data_processed['latent'][:ntsne] #[data_mask]\n",
    "data_index_subset = data_processed['clust_index'][:ntsne] #[data_mask]\n",
    "data_label_subset = data_processed['labels'][:ntsne]#[data_mask]\n",
    "data_alpha_subset = data_processed['clust_max'][:ntsne]#[data_mask]\n",
    "start = time.process_time() \n",
    "#data_tsne_results = run_tsne_skl(data_latent_subset, data_index_subset, data_alpha_subset, perp, exag, lr)\n",
    "print(\"Time:\", time.process_time() - start)\n",
    "#print(\"t-SNE output min/max:\", data_tsne_results.min(), data_tsne_results.max())\n",
    "#print(\"t-SNE output std per dim:\", data_tsne_results.std(axis=0))\n",
    "\n",
    "#sim_latent_subset = sim_latent_vect[:50000].copy()\n",
    "#sim_index_subset = sim_clust_index[:50000].copy()\n",
    "#sim_label_subset = sim_label_vect[:50000].copy()\n",
    "#sim_alpha_subset = sim_clust_max[:50000].copy()\n",
    "#sim_tsne_results = run_tsne_cuml(sim_latent_subset, sim_index_subset, perp, exag, lr,alpha_vect=sim_alpha_subset)\n",
    "#run_tsne_results = run_tsne_cuml(tsne_results=sim_tsne_results, zvect=sim_label_subset, ztitle=\"True label\", norm=False)\n",
    "\n",
    "## Merged\n",
    "#mixed_latent_subset = np.concatenate((data_latent_subset, sim_latent_subset), axis=0)\n",
    "#mixed_index_subset = np.concatenate((data_index_subset, sim_index_subset), axis=0)\n",
    "#mixed_label_subset = np.concatenate((data_label_subset, sim_label_subset), axis=0)\n",
    "#mixed_alpha_subset = np.concatenate((data_alpha_subset, sim_alpha_subset), axis=0)\n",
    "#mixed_tsne_results = run_tsne_cuml(mixed_latent_subset, mixed_index_subset, perp, exag, lr, mixed_alpha_subset)\n",
    "#run_tsne_results = run_tsne_cuml(tsne_results=mixed_tsne_results, zvect=mixed_label_subset, ztitle=\"True label\", norm=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perp=150\n",
    "exag=12\n",
    "lr=500.0\n",
    "data_tsne_results = run_tsne_skl(data_latent_subset, data_index_subset, data_alpha_subset, perp, exag, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Actually run tsne (not always that useful)\n",
    "perp=30\n",
    "exag=24\n",
    "lr=200.0\n",
    "\n",
    "for perp in [20, 30, 40, 50, 100]:\n",
    "    for exag in [8, 12, 16, 20]:\n",
    "        print(perp, exag)\n",
    "        data_tsne_results = run_tsne_skl(data_latent_subset, data_index_subset, data_alpha_subset, perp, exag, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perp=100\n",
    "exag=20\n",
    "lr=500\n",
    "n_iter=2000\n",
    "for lr in [50, 10000]: #200, 400, 600, 800, 1000, 2000, 5000]:\n",
    "    print(\"Trying n_iter =\", n_iter)\n",
    "    data_tsne_results = run_tsne_skl(data_latent_subset, data_index_subset, data_alpha_subset, perp, exag, lr, n_iter=n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def run_umap_cpu(input_vect=None, zvect=None, n_neighbors=100, min_distance=0.1, n_epochs=1000, alpha_vect=0.5, ztitle=\"Cluster ID\", save_name=None, norm=True):\n",
    "    \n",
    "    if norm:\n",
    "        norms = np.linalg.norm(input_vect, axis=1, keepdims=True)\n",
    "        input_vect = input_vect / (norms + 1e-10)\n",
    "\n",
    "    fit = umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_distance,\n",
    "        n_epochs=n_epochs,\n",
    "        metric='cosine',\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    umap_results = fit.fit_transform(input_vect)    \n",
    "\n",
    "    x_low, x_high = np.percentile(umap_results[:,0], [0.1, 99.9])\n",
    "    y_low, y_high = np.percentile(umap_results[:,1], [0.1, 99.9])\n",
    "    \n",
    "    unique_labels = np.unique(zvect)\n",
    "    n_clusters = len(unique_labels)\n",
    "\n",
    "    # Use a qualitative colormap with enough colors\n",
    "    all_colors = (\n",
    "        plt.cm.tab20.colors +\n",
    "        plt.cm.tab20b.colors +\n",
    "        plt.cm.tab20c.colors +\n",
    "        plt.cm.tab10.colors\n",
    "    )\n",
    "\n",
    "    cmap = mcolors.ListedColormap(all_colors[:n_clusters])\n",
    "    norm = mcolors.BoundaryNorm(boundaries=np.arange(n_clusters + 1), ncolors=n_clusters)\n",
    "\n",
    "    gr = plt.scatter(umap_results[:, 0], umap_results[:, 1], s=0.5, alpha=alpha_vect, c=zvect, cmap=cmap, norm=norm)\n",
    "    plt.colorbar(gr, label=ztitle)\n",
    "    plt.xlim(x_low, x_high)\n",
    "    plt.ylim(y_low, y_high)\n",
    "    plt.xlabel('UMAP #0')\n",
    "    plt.ylabel('UMAP #1')\n",
    "    ax = plt.gca()\n",
    "    ax.grid(False)\n",
    "    if save_name: plt.savefig(save_name, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    #plt.close()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numap=20000\n",
    "for n_neighbors in [5, 10, 20, 30, 50]:\n",
    "    for min_distance in [0, 0.01, 0.05, 0.1]:\n",
    "        print(n_neighbors, min_distance)\n",
    "        run_umap_cpu(data_processed['latent'][:numap], data_processed['clust_index'][:numap], n_neighbors=n_neighbors, min_distance=min_distance, \\\n",
    "                     alpha_vect=data_processed[\"clust_max\"][:numap], n_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors=50\n",
    "for min_distance in [0.001, 0.05, 0.01]:\n",
    "    for n_epochs in [500, 1000, 2000, 5000, 10000]:\n",
    "        print(n_neighbors, min_distance, n_epochs)\n",
    "        run_umap_cpu(data_processed['latent'][:numap], data_processed['clust_index'][:numap], n_neighbors=n_neighbors, min_distance=min_distance, \\\n",
    "                     alpha_vect=data_processed[\"clust_max\"][:numap], n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple averages:\n",
    "data_counts = np.bincount(data_clust_index)\n",
    "sim_counts = np.bincount(sim_clust_index)\n",
    "n_clusters = 20\n",
    "data_frac = data_counts / data_counts.sum()\n",
    "sim_frac = sim_counts / sim_counts.sum()\n",
    "\n",
    "# per-cluster mean confidence\n",
    "data_mean_conf = np.zeros(n_clusters)\n",
    "sim_mean_conf = np.zeros(n_clusters)\n",
    "\n",
    "for k in range(n_clusters):\n",
    "    data_mean_conf[k] = data_clust_vect[data_clust_index==k, :].max(axis=1).mean()\n",
    "    sim_mean_conf[k] = sim_clust_vect[sim_clust_index==k, :].max(axis=1).mean()\n",
    "\n",
    "print(\"Data cluster sizes:\", data_counts)\n",
    "print(\"Data fraction > 0.01:\", (data_frac>0.01).sum())\n",
    "print(\"Data mean max-prob per cluster:\", data_mean_conf)\n",
    "\n",
    "print(\"Sim cluster sizes:\", sim_counts)\n",
    "print(\"Sim fraction > 0.01:\", (sim_frac>0.01).sum())\n",
    "print(\"Sim mean max-prob per cluster:\", sim_mean_conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Actually run tsne (not always that useful)\n",
    "perp=30\n",
    "exag=24\n",
    "lr=200.0\n",
    "\n",
    "for perp in [30, 40, 50, 100]:\n",
    "    for exag in [8, 12, 16, 20]:\n",
    "        print(perp, exag)\n",
    "        data_tsne_results = run_tsne_skl(data_latent_subset, data_index_subset, data_alpha_subset, perp, exag, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to make column normalized histograms\n",
    "from matplotlib.colors import LogNorm\n",
    "def make_2D_histogram(x_vect, y_vect, norm='column', label_enum=Label):\n",
    "    # Determine range of unique integer values\n",
    "    x_min, x_max = x_vect.min(), x_vect.max()\n",
    "    y_min, y_max = y_vect.min(), y_vect.max()\n",
    "\n",
    "    # Define bin edges so each integer gets its own bin\n",
    "    x_bins = np.arange(x_min, x_max + 2)  # +2 to include the last integer\n",
    "    y_bins = np.arange(y_min, y_max + 2)\n",
    "\n",
    "    # Compute the 2D histogram\n",
    "    H, xedges, yedges = np.histogram2d(x_vect, y_vect, bins=[x_bins, y_bins])\n",
    "    H = H.T\n",
    "    \n",
    "    # Column normalization: divide each column by its sum\n",
    "    # Note: H shape is (len(x_bins)-1, len(y_bins)-1)\n",
    "    column_sums = H.sum(axis=0, keepdims=True)\n",
    "    row_sums = H.sum(axis=1, keepdims=True)\n",
    "\n",
    "    \n",
    "    if norm=='column': \n",
    "        H_normalized = np.divide(H, column_sums, where=column_sums != 0)\n",
    "    elif norm=='row':\n",
    "        H_normalized = np.divide(H, row_sums, where=row_sums != 0)\n",
    "    else:\n",
    "        print(\"Unknown norm option:, norm\")\n",
    "        return\n",
    "        \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    mesh = plt.pcolormesh(x_bins, y_bins, H_normalized, cmap='viridis', shading='auto')\n",
    "    plt.colorbar(mesh, label='Normalized Frequency (per '+norm+')')\n",
    "    plt.ylabel(\"Cluster ID\")\n",
    "\n",
    "    if label_enum is not None:\n",
    "        x_ticks = np.arange(x_min, x_max + 1)\n",
    "        x_labels = [label_enum.name_from_index(i) for i in x_ticks]\n",
    "        plt.xticks(ticks=x_ticks + 1, labels=x_labels, rotation=45, ha='right')\n",
    "    plt.yticks(ticks=y_bins[:-1])\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_2D_histogram(label_vect, clust_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_2D_histogram(label_vect, clust_index, 'row')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ME_analysis_libs\n",
    "importlib.reload(ME_analysis_libs)\n",
    "from ME_analysis_libs import plot_cluster_examples\n",
    "## Now pull out a bank of example images for each cluster\n",
    "for index in range(nclusters):\n",
    "    print(\"Showing examples for cluster:\", index, \"which has\", np.count_nonzero(data_processed['clust_index']==index), \"values\")\n",
    "    plot_cluster_examples(single_data_dataset, data_processed['clust_index'], index, 8)#, data_processed['clust_max'])\n",
    "\n",
    "    ## Or simulation\n",
    "    #print(\"Showing examples for cluster:\", index, \"which has\", np.count_nonzero(sim_processed['clust_index']==index), \"values\")\n",
    "    #plot_cluster_examples(single_sim_dataset, sim_processed['clust_index'], index, 8, data_processed['clust_max'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dump out a large block of images for one cluster\n",
    "from ME_analysis_libs import plot_cluster_bigblock\n",
    "plot_cluster_bigblock(single_data_dataset, data_processed['clust_index'], 18, 10, 10, cluster_probs=data_processed['clust_max']) #, 'cluster_plots/v9_michel_like.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "## Dump out a file including the filenames and indices for the clustered images (for going back to the original files)\n",
    "def dump_cluster_indices(index_label, cluster_labels, filenames, event_ids):\n",
    "\n",
    "    # Inputs\n",
    "    indices = np.where(cluster_labels == index_label)[0]\n",
    "\n",
    "    selected_filenames = np.array(filenames)[indices]\n",
    "    selected_event_ids = np.array(event_ids)[indices]\n",
    "\n",
    "    # Group by filename\n",
    "    grouped = defaultdict(list)\n",
    "    for fname, eid in zip(selected_filenames, selected_event_ids):\n",
    "        ## Restore the old naming for ease of interpretation\n",
    "        grouped[fname.replace(\"_images.h5\", \".hdf5\")].append(int(eid))  # ensure JSON serializability\n",
    "\n",
    "    # Save to JSON\n",
    "    output_file = f'cluster_{index_label}_events.json'\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(grouped, f, indent=2)\n",
    "\n",
    "    print(f\"Saved grouped event list for cluster {index_label} to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_cluster_indices(16, sim_processed['clust_index'], sim_processed[\"filename\"], sim_processed[\"event_id\"])\n",
    "dump_cluster_indices(17, sim_processed['clust_index'], sim_processed[\"filename\"], sim_processed[\"event_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_tools_ME",
   "language": "python",
   "name": "ml_tools_me"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
